{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "final_notebook_2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "traditional-insulin",
        "designing-pickup",
        "tropical-guest",
        "agreed-webster",
        "varying-constant",
        "adult-jason",
        "heated-thread",
        "potential-affiliation",
        "alert-effects"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "charged-snowboard"
      },
      "source": [
        "# World Data League 2021\n",
        "## Notebook Template\n",
        "\n",
        "This notebook is one of the mandatory deliverables when you submit your solution (alongside the video pitch). Its structure follows the WDL evaluation criteria and it has dedicated cells where you can add descriptions. Make sure your code is readable as it will be the only technical support the jury will have to evaluate your work.\n",
        "\n",
        "The notebook must:\n",
        "\n",
        "*   üíª have all the code that you want the jury to evaluate\n",
        "*   üß± follow the predefined structure\n",
        "*   üìÑ have markdown descriptions where you find necessary\n",
        "*   üëÄ be saved with all the output that you want the jury to see\n",
        "*   üèÉ‚Äç‚ôÇÔ∏è be runnable\n"
      ],
      "id": "charged-snowboard"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-03T10:46:52.146959Z",
          "start_time": "2021-07-03T10:46:52.128960Z"
        },
        "id": "breathing-contamination"
      },
      "source": [
        "## Authors\n",
        "- Nicholas Sistovaris\n",
        "- Moritz Geiger\n",
        "- Pravalika Myneni\n",
        "- Sowmya Madela"
      ],
      "id": "breathing-contamination"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "million-florida"
      },
      "source": [
        "## External links and resources"
      ],
      "id": "million-florida"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-03T10:56:16.294657Z",
          "start_time": "2021-07-03T10:56:16.286392Z"
        },
        "id": "domestic-finland"
      },
      "source": [
        "All the external data or resources that was not provided by the WDL was acquired through the following links:\n",
        "\n",
        "1. https://noise-planet.org/noisemodelling.html \n",
        "2. https://www.torinocitylab.it/en/asset-to/open-data \n",
        "3. https://www.officeholidays.com/countries/italy/turin/2018 \n",
        "4. https://www.feiertagskalender.ch/index.php?geo=3815&jahr=2018&hl=en\n",
        "5. http://webgis.arpa.piemonte.it/basicviewer_arpa_webapp/index.html?webmap=89aa175451d24ae0a1911e67957d9aec\n",
        "6. http://aperto.comune.torino.it/dataset/zone-statistiche\n",
        "7. https://openweathermap.org/history\n",
        "8. https://developers.google.com/maps/documentation/places/web-service/details "
      ],
      "id": "domestic-finland"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "british-ecology"
      },
      "source": [
        "## Introduction"
      ],
      "id": "british-ecology"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opened-hypothetical"
      },
      "source": [
        "**Overview:**\n",
        "\n",
        "\n",
        "_from challenge description_\n",
        "<blockquote>\n",
        "\n",
        "</blockquote>\n",
        "\n"
      ],
      "id": "opened-hypothetical"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sensitive-region"
      },
      "source": [
        "**Research:**\n",
        "\n"
      ],
      "id": "sensitive-region"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dated-discovery"
      },
      "source": [
        "## Development\n",
        "Start coding here! üë©‚Äçüíª\n",
        "\n",
        "Don't hesitate to create markdown cells to include descriptions of your work where you see fit, as well as commenting your code.\n",
        "\n",
        "We know that you know exactly where to start when it comes to crunching data and building models, but don't forget that WDL is all about social impact...so take that into consideration as well."
      ],
      "id": "dated-discovery"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "significant-liability"
      },
      "source": [
        "### Imports (libraries) üìö"
      ],
      "id": "significant-liability"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-07-03T11:17:08.517645Z",
          "start_time": "2021-07-03T11:17:02.250175Z"
        },
        "id": "creative-marketplace"
      },
      "source": [
        "## TABULAR\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "## GEO\n",
        "import geopandas as gpd\n",
        "import fiona\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster, HeatMap, BeautifyIcon\n",
        "from folium.map import LayerControl, Layer, FeatureGroup\n",
        "from folium.vector_layers import Circle, CircleMarker\n",
        "from shapely.geometry import LineString, Point\n",
        "from shapely import wkt\n",
        "\n",
        "\n",
        "## DATA\n",
        "import os\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "import re\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import ast\n",
        "import datetime as dt\n",
        "from io import StringIO, BytesIO\n",
        "\n",
        "\n",
        "## VIS\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.tsa\n",
        "import branca\n",
        "import plotly.express as px\n",
        "\n",
        "## TIME SERIES\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
        "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
        "import statsmodels.tsa.api as smt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from pmdarima.arima import auto_arima \n",
        "\n",
        "\n",
        "## MODELLING\n",
        "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso \n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor as rfr\n",
        "\n",
        "## NEURAL NETWORKS\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout"
      ],
      "id": "creative-marketplace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "third-cylinder"
      },
      "source": [
        "### Importing Dataframes"
      ],
      "id": "third-cylinder"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "difficult-wedding"
      },
      "source": [
        "Following a first glance at the dataframes provided by the WDL, we believed that using data from **2018** was our best bet to construct our model on. \n",
        "\n",
        "- First, we wanted to focus on understanding noise and complaints in the pre-covid context. The years 2020 and 2021 would have been unrepresentative of Turin's nightlife.\n",
        "\n",
        "- Secondly, we wanted a feature that would represent the number of people outsides on an hourly basis. The data on No. of Visitors based on WiFi was most complete and representative of the population outside. However, it only had data for October, November & December 2018. This is why we picked 2018 for the rest of our data."
      ],
      "id": "difficult-wedding"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "valuable-theorem",
        "outputId": "56c2d385-254a-4089-d018-d10831185edb"
      },
      "source": [
        "# location of the sensors\n",
        "df_sensors_def = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/noise_sensor_list.csv', sep=';')\n",
        "df_sensors_def"
      ],
      "id": "valuable-theorem",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>address</th>\n",
              "      <th>Lat</th>\n",
              "      <th>Long</th>\n",
              "      <th>streaming</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>s_01</td>\n",
              "      <td>Via Saluzzo, 26 Torino</td>\n",
              "      <td>45,059172</td>\n",
              "      <td>7,678986</td>\n",
              "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>s_02</td>\n",
              "      <td>Via Principe Tommaso, 18bis Torino</td>\n",
              "      <td>45,057837</td>\n",
              "      <td>7,681555</td>\n",
              "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>s_03</td>\n",
              "      <td>Largo Saluzzo Torino</td>\n",
              "      <td>45,058518</td>\n",
              "      <td>7,678854</td>\n",
              "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>s_05</td>\n",
              "      <td>Via Principe Tommaso angolo via Baretti Torino</td>\n",
              "      <td>45,057603</td>\n",
              "      <td>7,681348</td>\n",
              "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>s_06</td>\n",
              "      <td>Corso Marconi, 27 Torino</td>\n",
              "      <td>45,055554</td>\n",
              "      <td>7,68259</td>\n",
              "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   code                                         address        Lat      Long  \\\n",
              "0  s_01                          Via Saluzzo, 26 Torino  45,059172  7,678986   \n",
              "1  s_02              Via Principe Tommaso, 18bis Torino  45,057837  7,681555   \n",
              "2  s_03                            Largo Saluzzo Torino  45,058518  7,678854   \n",
              "3  s_05  Via Principe Tommaso angolo via Baretti Torino  45,057603  7,681348   \n",
              "4  s_06                        Corso Marconi, 27 Torino  45,055554   7,68259   \n",
              "\n",
              "                                           streaming  \n",
              "0  https://userportal.smartdatanet.it/userportal/...  \n",
              "1  https://userportal.smartdatanet.it/userportal/...  \n",
              "2  https://userportal.smartdatanet.it/userportal/...  \n",
              "3  https://userportal.smartdatanet.it/userportal/...  \n",
              "4  https://userportal.smartdatanet.it/userportal/...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unable-pavilion"
      },
      "source": [
        "**Note** The location of sensors was optimized to cover all\n",
        "significant feature of ‚ÄúMovida‚Äù area:\n",
        "one in a very crowded square (S_03, not active in\n",
        "daytime), three in narrow streets with pubs and\n",
        "bars (S_01, S_04, S_05), one in a boulevard for\n",
        "traffic noise measurement (S_06) and the last one\n",
        "in a quieter area with no crowd and low traffic\n",
        "(S_02), for global reference. The choice of points\n",
        "of installation was driven also by the power\n",
        "supply, so light poles, public offices and bike\n",
        "sharing station where preferred.\n",
        "\n",
        "Source: https://wdl-data.fra1.digitaloceanspaces.com/torino/120_Euronoise2018.pdf"
      ],
      "id": "unable-pavilion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "optional-caribbean",
        "outputId": "be23213c-03f8-40b0-c950-ef4faad7bfac"
      },
      "source": [
        "df_wifi = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/WIFI%20Count.csv', sep=',')\n",
        "df_wifi"
      ],
      "id": "optional-caribbean",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>No. of Visitors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-10-24 17:00</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-10-24 18:00</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-10-24 19:00</td>\n",
              "      <td>181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-10-24 20:00</td>\n",
              "      <td>211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-10-24 21:00</td>\n",
              "      <td>239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1634</th>\n",
              "      <td>2018-12-31 19:00</td>\n",
              "      <td>158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1635</th>\n",
              "      <td>2018-12-31 20:00</td>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1636</th>\n",
              "      <td>2018-12-31 21:00</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1637</th>\n",
              "      <td>2018-12-31 22:00</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1638</th>\n",
              "      <td>2018-12-31 23:00</td>\n",
              "      <td>168</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1639 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Time  No. of Visitors\n",
              "0     2018-10-24 17:00               47\n",
              "1     2018-10-24 18:00              155\n",
              "2     2018-10-24 19:00              181\n",
              "3     2018-10-24 20:00              211\n",
              "4     2018-10-24 21:00              239\n",
              "...                ...              ...\n",
              "1634  2018-12-31 19:00              158\n",
              "1635  2018-12-31 20:00              171\n",
              "1636  2018-12-31 21:00              151\n",
              "1637  2018-12-31 22:00              125\n",
              "1638  2018-12-31 23:00              168\n",
              "\n",
              "[1639 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "composite-entertainment"
      },
      "source": [
        "**Note** As you can see, from the data above, we can get an idea of the number of people outside at different hours."
      ],
      "id": "composite-entertainment"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "beneficial-ground",
        "outputId": "f749399d-a69e-4d41-a616-b480deb79327"
      },
      "source": [
        "df_businesses = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/businesses.csv', sep=';')\n",
        "df_businesses.head()"
      ],
      "id": "beneficial-ground",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WKT</th>\n",
              "      <th>ADDRESS</th>\n",
              "      <th>OPEN YEAR</th>\n",
              "      <th>OPEN MONTH</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>Description</th>\n",
              "      <th>Merchandise Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POINT (1396322.217 4990301.69)</td>\n",
              "      <td>VIA CLAUDIO LUIGI BERTHOLLET 24</td>\n",
              "      <td>1977</td>\n",
              "      <td>1</td>\n",
              "      <td>EXTRALIMENTARI</td>\n",
              "      <td>PICCOLE STRUTTURE</td>\n",
              "      <td>Extralimentari</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>POINT (1396322.217 4990301.69)</td>\n",
              "      <td>VIA CLAUDIO LUIGI BERTHOLLET 24</td>\n",
              "      <td>1985</td>\n",
              "      <td>6</td>\n",
              "      <td>ALIMENTARI</td>\n",
              "      <td>PICCOLE STRUTTURE</td>\n",
              "      <td>Panificio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>POINT (1396303.762 4990325.001)</td>\n",
              "      <td>VIA CLAUDIO LUIGI BERTHOLLET 25/F</td>\n",
              "      <td>2017</td>\n",
              "      <td>9</td>\n",
              "      <td>ALTRO</td>\n",
              "      <td>DIA di somministrazione</td>\n",
              "      <td>Nessuna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>POINT (1396434.395 4990540.6)</td>\n",
              "      <td>CORSO VITTORIO EMANUELE II 21/A</td>\n",
              "      <td>2013</td>\n",
              "      <td>10</td>\n",
              "      <td>ALTRO</td>\n",
              "      <td>DIA di somministrazione</td>\n",
              "      <td>Nessuna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>POINT (1396434.395 4990540.6)</td>\n",
              "      <td>CORSO VITTORIO EMANUELE II 21/A</td>\n",
              "      <td>2009</td>\n",
              "      <td>2</td>\n",
              "      <td>ALTRO</td>\n",
              "      <td>DIA di somministrazione</td>\n",
              "      <td>Nessuna</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               WKT                            ADDRESS  \\\n",
              "0   POINT (1396322.217 4990301.69)    VIA CLAUDIO LUIGI BERTHOLLET 24   \n",
              "1   POINT (1396322.217 4990301.69)    VIA CLAUDIO LUIGI BERTHOLLET 24   \n",
              "2  POINT (1396303.762 4990325.001)  VIA CLAUDIO LUIGI BERTHOLLET 25/F   \n",
              "3    POINT (1396434.395 4990540.6)    CORSO VITTORIO EMANUELE II 21/A   \n",
              "4    POINT (1396434.395 4990540.6)    CORSO VITTORIO EMANUELE II 21/A   \n",
              "\n",
              "   OPEN YEAR  OPEN MONTH            TYPE              Description  \\\n",
              "0       1977           1  EXTRALIMENTARI        PICCOLE STRUTTURE   \n",
              "1       1985           6      ALIMENTARI        PICCOLE STRUTTURE   \n",
              "2       2017           9           ALTRO  DIA di somministrazione   \n",
              "3       2013          10           ALTRO  DIA di somministrazione   \n",
              "4       2009           2           ALTRO  DIA di somministrazione   \n",
              "\n",
              "  Merchandise Type  \n",
              "0   Extralimentari  \n",
              "1        Panificio  \n",
              "2          Nessuna  \n",
              "3          Nessuna  \n",
              "4          Nessuna  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "specialized-flashing"
      },
      "source": [
        "**Note** Location & Description of various businesses "
      ],
      "id": "specialized-flashing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tamil-interaction",
        "outputId": "fc5adb96-4d2c-47e7-a974-0d6cfd71a241"
      },
      "source": [
        "df_sim_june = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/sim_count/SIM_count_04_100618.csv', sep=';', encoding='latin-1')\n",
        "df_sim_june.head()"
      ],
      "id": "tamil-interaction",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cluster</th>\n",
              "      <th>data_da</th>\n",
              "      <th>data_a</th>\n",
              "      <th>numero_presenze</th>\n",
              "      <th>layer_id</th>\n",
              "      <th>layer_nome</th>\n",
              "      <th>dettaglio(secondi)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-06-10T21:00:00Z</td>\n",
              "      <td>2018-06-10T22:00:00Z</td>\n",
              "      <td>3278</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-06-10T20:00:00Z</td>\n",
              "      <td>2018-06-10T21:00:00Z</td>\n",
              "      <td>3324</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-06-10T19:00:00Z</td>\n",
              "      <td>2018-06-10T20:00:00Z</td>\n",
              "      <td>3318</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-06-10T18:00:00Z</td>\n",
              "      <td>2018-06-10T19:00:00Z</td>\n",
              "      <td>3187</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-06-10T17:00:00Z</td>\n",
              "      <td>2018-06-10T18:00:00Z</td>\n",
              "      <td>2980</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    cluster               data_da                data_a  numero_presenze  \\\n",
              "0  Presenze  2018-06-10T21:00:00Z  2018-06-10T22:00:00Z             3278   \n",
              "1  Presenze  2018-06-10T20:00:00Z  2018-06-10T21:00:00Z             3324   \n",
              "2  Presenze  2018-06-10T19:00:00Z  2018-06-10T20:00:00Z             3318   \n",
              "3  Presenze  2018-06-10T18:00:00Z  2018-06-10T19:00:00Z             3187   \n",
              "4  Presenze  2018-06-10T17:00:00Z  2018-06-10T18:00:00Z             2980   \n",
              "\n",
              "                               layer_id layer_nome  dettaglio(secondi)  \n",
              "0  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "1  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "2  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "3  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "4  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "handy-serum",
        "outputId": "46d5292b-f8b0-4a40-98ea-ff32013d9091"
      },
      "source": [
        "df_sim_jan = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/sim_count/SIM_count_15_210118.csv', sep=';', encoding='latin-1')\n",
        "df_sim_jan.head()"
      ],
      "id": "handy-serum",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cluster</th>\n",
              "      <th>data_da</th>\n",
              "      <th>data_a</th>\n",
              "      <th>numero_presenze</th>\n",
              "      <th>layer_id</th>\n",
              "      <th>layer_nome</th>\n",
              "      <th>dettaglio(secondi)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-01-21T22:00:00Z</td>\n",
              "      <td>2018-01-21T23:00:00Z</td>\n",
              "      <td>3026</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-01-21T21:00:00Z</td>\n",
              "      <td>2018-01-21T22:00:00Z</td>\n",
              "      <td>3088</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-01-21T20:00:00Z</td>\n",
              "      <td>2018-01-21T21:00:00Z</td>\n",
              "      <td>3119</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-01-21T19:00:00Z</td>\n",
              "      <td>2018-01-21T20:00:00Z</td>\n",
              "      <td>3114</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-01-21T18:00:00Z</td>\n",
              "      <td>2018-01-21T19:00:00Z</td>\n",
              "      <td>2991</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    cluster               data_da                data_a  numero_presenze  \\\n",
              "0  Presenze  2018-01-21T22:00:00Z  2018-01-21T23:00:00Z             3026   \n",
              "1  Presenze  2018-01-21T21:00:00Z  2018-01-21T22:00:00Z             3088   \n",
              "2  Presenze  2018-01-21T20:00:00Z  2018-01-21T21:00:00Z             3119   \n",
              "3  Presenze  2018-01-21T19:00:00Z  2018-01-21T20:00:00Z             3114   \n",
              "4  Presenze  2018-01-21T18:00:00Z  2018-01-21T19:00:00Z             2991   \n",
              "\n",
              "                               layer_id layer_nome  dettaglio(secondi)  \n",
              "0  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "1  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "2  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "3  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "4  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "national-dominant",
        "outputId": "230c4535-09a9-4313-ab21-06fdb37562a2"
      },
      "source": [
        "df_sim_march = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/sim_count/SIM_count_19_250318.csv', sep=';', encoding='latin-1')\n",
        "df_sim_march.head()"
      ],
      "id": "national-dominant",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cluster</th>\n",
              "      <th>data_da</th>\n",
              "      <th>data_a</th>\n",
              "      <th>numero_presenze</th>\n",
              "      <th>layer_id</th>\n",
              "      <th>layer_nome</th>\n",
              "      <th>dettaglio(secondi)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-03-25T21:00:00Z</td>\n",
              "      <td>2018-03-25T22:00:00Z</td>\n",
              "      <td>3267</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-03-25T20:00:00Z</td>\n",
              "      <td>2018-03-25T21:00:00Z</td>\n",
              "      <td>3373</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-03-25T19:00:00Z</td>\n",
              "      <td>2018-03-25T20:00:00Z</td>\n",
              "      <td>3410</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-03-25T18:00:00Z</td>\n",
              "      <td>2018-03-25T19:00:00Z</td>\n",
              "      <td>3358</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Presenze</td>\n",
              "      <td>2018-03-25T17:00:00Z</td>\n",
              "      <td>2018-03-25T18:00:00Z</td>\n",
              "      <td>3229</td>\n",
              "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
              "      <td>Area 1</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    cluster               data_da                data_a  numero_presenze  \\\n",
              "0  Presenze  2018-03-25T21:00:00Z  2018-03-25T22:00:00Z             3267   \n",
              "1  Presenze  2018-03-25T20:00:00Z  2018-03-25T21:00:00Z             3373   \n",
              "2  Presenze  2018-03-25T19:00:00Z  2018-03-25T20:00:00Z             3410   \n",
              "3  Presenze  2018-03-25T18:00:00Z  2018-03-25T19:00:00Z             3358   \n",
              "4  Presenze  2018-03-25T17:00:00Z  2018-03-25T18:00:00Z             3229   \n",
              "\n",
              "                               layer_id layer_nome  dettaglio(secondi)  \n",
              "0  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "1  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "2  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "3  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
              "4  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unauthorized-coordination"
      },
      "source": [
        "df_sim_all = pd.concat([df_sim_jan, df_sim_march, df_sim_june], axis=0)\n",
        "df_sim_all.reset_index(inplace=True)"
      ],
      "id": "unauthorized-coordination",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "structured-liabilities"
      },
      "source": [
        "**Note** Another possibility to estimate the number of people outside at certain hours is the SIM card dataframes. What it highlights is the presence of certain SIM card users at different hours of the day. We have access to SIM card data of 2018 for January, March and June."
      ],
      "id": "structured-liabilities"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "chronic-romance",
        "outputId": "a5ab75bc-8a17-42ee-d049-99b385aaaa9d"
      },
      "source": [
        "df_noise_2018 = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/noise_data/san_salvario_2018.csv', skiprows= [0,1,2,3,4,5,6,7], sep =';')\n",
        "df_noise_2018.head()"
      ],
      "id": "chronic-romance",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "      <th>Ora</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5,,,,,</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01-01-2018</td>\n",
              "      <td>00:00</td>\n",
              "      <td>68,7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>76,0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>66,6,,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01-01-2018</td>\n",
              "      <td>01:00</td>\n",
              "      <td>68,3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>68,2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>65,4,,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01-01-2018</td>\n",
              "      <td>02:00</td>\n",
              "      <td>59,8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>64,4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>64,4,,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01-01-2018</td>\n",
              "      <td>03:00</td>\n",
              "      <td>67,4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>67,5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>61,8,,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01-01-2018</td>\n",
              "      <td>04:00</td>\n",
              "      <td>68,0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>64,5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>60,5,,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Data    Ora    C1   C2    C3   C4 C5,,,,,\n",
              "0  01-01-2018  00:00  68,7  NaN  76,0  NaN  66,6,,\n",
              "1  01-01-2018  01:00  68,3  NaN  68,2  NaN  65,4,,\n",
              "2  01-01-2018  02:00  59,8  NaN  64,4  NaN  64,4,,\n",
              "3  01-01-2018  03:00  67,4  NaN  67,5  NaN  61,8,,\n",
              "4  01-01-2018  04:00  68,0  NaN  64,5  NaN  60,5,,"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "confidential-advocacy"
      },
      "source": [
        "**Note** The noise data is records of noice measurements using 5 different sensors spread in the San Salvario region on an hourly basis. We will use this data as our target in our time series measurements. "
      ],
      "id": "confidential-advocacy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "southern-emphasis",
        "outputId": "4178dc45-ac1b-41dc-e7e6-f347b256360d"
      },
      "source": [
        "df_police_1 = pd.read_excel('https://github.com/McNickSisto/world_data_league/blob/main/stage_final/data/police_complaints/OpenDataContact_Gennaio_Giugno_2018.xlsx?raw=true')\n",
        "df_police_1.head()"
      ],
      "id": "southern-emphasis",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Categoria criminologa</th>\n",
              "      <th>Sottocategoria Criminologica</th>\n",
              "      <th>Circoscrizione</th>\n",
              "      <th>Localita</th>\n",
              "      <th>Area Verde</th>\n",
              "      <th>Data</th>\n",
              "      <th>Ora</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>6.0</td>\n",
              "      <td>BELMONTE/(VIA)                                ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>01/02/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>6.0</td>\n",
              "      <td>DONATORE DI SANGUE/(PIAZZA DEL)               ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12/02/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>4.0</td>\n",
              "      <td>CIBRARIO/LUIGI (VIA)                          ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26/02/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ROMA/(VIA)                                    ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>02/03/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>4.0</td>\n",
              "      <td>ZUMAGLIA/(VIA)                                ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>05/03/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Categoria criminologa Sottocategoria Criminologica  Circoscrizione  \\\n",
              "0       Allarme Sociale                        Altro             6.0   \n",
              "1       Allarme Sociale                        Altro             6.0   \n",
              "2       Allarme Sociale                        Altro             4.0   \n",
              "3       Allarme Sociale                        Altro             1.0   \n",
              "4       Allarme Sociale                        Altro             4.0   \n",
              "\n",
              "                                            Localita Area Verde        Data  \\\n",
              "0  BELMONTE/(VIA)                                ...        NaN  01/02/2018   \n",
              "1  DONATORE DI SANGUE/(PIAZZA DEL)               ...        NaN  12/02/2018   \n",
              "2  CIBRARIO/LUIGI (VIA)                          ...        NaN  26/02/2018   \n",
              "3  ROMA/(VIA)                                    ...        NaN  02/03/2018   \n",
              "4  ZUMAGLIA/(VIA)                                ...        NaN  05/03/2018   \n",
              "\n",
              "   Ora  \n",
              "0  NaN  \n",
              "1  NaN  \n",
              "2  NaN  \n",
              "3  NaN  \n",
              "4  NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inclusive-renaissance",
        "outputId": "be191896-d8a1-408a-b1b0-4713cfa862a0"
      },
      "source": [
        "df_police_2 = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/police_complaints/OpenDataContact_Luglio_Dicembre_2018.csv')\n",
        "df_police_2.head()"
      ],
      "id": "inclusive-renaissance",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Categoria criminologa</th>\n",
              "      <th>Sottocategoria Criminologica</th>\n",
              "      <th>Circoscrizione</th>\n",
              "      <th>Localita</th>\n",
              "      <th>Area Verde</th>\n",
              "      <th>Data</th>\n",
              "      <th>Ora</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>8.0</td>\n",
              "      <td>D'AZEGLIO/MASSIMO (CORSO)                     ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16/07/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>1.0</td>\n",
              "      <td>REGINA MARGHERITA/(CORSO)                     ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17/07/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>10.0</td>\n",
              "      <td>DUINO/(VIA)                                   ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14/09/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>02/10/2018</td>\n",
              "      <td>9.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>9.0</td>\n",
              "      <td>CARDUCCI/GIOSUE' (PIAZZA)                     ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>27/11/2018</td>\n",
              "      <td>11.53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Categoria criminologa Sottocategoria Criminologica  Circoscrizione  \\\n",
              "0       Allarme Sociale                        Altro             8.0   \n",
              "1       Allarme Sociale                        Altro             1.0   \n",
              "2       Allarme Sociale                        Altro            10.0   \n",
              "3       Allarme Sociale                        Altro             NaN   \n",
              "4       Allarme Sociale                        Altro             9.0   \n",
              "\n",
              "                                            Localita Area Verde        Data  \\\n",
              "0  D'AZEGLIO/MASSIMO (CORSO)                     ...        NaN  16/07/2018   \n",
              "1  REGINA MARGHERITA/(CORSO)                     ...        NaN  17/07/2018   \n",
              "2  DUINO/(VIA)                                   ...        NaN  14/09/2018   \n",
              "3                                                NaN        NaN  02/10/2018   \n",
              "4  CARDUCCI/GIOSUE' (PIAZZA)                     ...        NaN  27/11/2018   \n",
              "\n",
              "     Ora  \n",
              "0    NaN  \n",
              "1    NaN  \n",
              "2    NaN  \n",
              "3   9.40  \n",
              "4  11.53  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frozen-poland",
        "outputId": "41fb994a-f2bb-4524-859f-bffd9fcca5c6"
      },
      "source": [
        "df_police = pd.concat([df_police_1,df_police_2])\n",
        "df_police"
      ],
      "id": "frozen-poland",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Categoria criminologa</th>\n",
              "      <th>Sottocategoria Criminologica</th>\n",
              "      <th>Circoscrizione</th>\n",
              "      <th>Localita</th>\n",
              "      <th>Area Verde</th>\n",
              "      <th>Data</th>\n",
              "      <th>Ora</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>6.0</td>\n",
              "      <td>BELMONTE/(VIA)                                ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>01/02/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>6.0</td>\n",
              "      <td>DONATORE DI SANGUE/(PIAZZA DEL)               ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12/02/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>4.0</td>\n",
              "      <td>CIBRARIO/LUIGI (VIA)                          ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26/02/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ROMA/(VIA)                                    ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>02/03/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Allarme Sociale</td>\n",
              "      <td>Altro</td>\n",
              "      <td>4.0</td>\n",
              "      <td>ZUMAGLIA/(VIA)                                ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>05/03/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>Qualit√† Urbana</td>\n",
              "      <td>Decoro e degrado urbano</td>\n",
              "      <td>6.0</td>\n",
              "      <td>VERCELLI/(CORSO)                              ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>31/12/2018</td>\n",
              "      <td>11.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>Qualit√† Urbana</td>\n",
              "      <td>Veicoli abbandonati</td>\n",
              "      <td>4.0</td>\n",
              "      <td>BOSELLI/PAOLO (VIA)                           ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17/09/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>Qualit√† Urbana</td>\n",
              "      <td>Veicoli abbandonati</td>\n",
              "      <td>4.0</td>\n",
              "      <td>PIFFETTI/PIETRO (VIA)                         ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22/09/2018</td>\n",
              "      <td>14.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>Qualit√† Urbana</td>\n",
              "      <td>Veicoli abbandonati</td>\n",
              "      <td>6.0</td>\n",
              "      <td>FOSSATA/(VIA)                                 ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22/09/2018</td>\n",
              "      <td>9.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>Qualit√† Urbana</td>\n",
              "      <td>Veicoli abbandonati</td>\n",
              "      <td>9.0</td>\n",
              "      <td>ROSARIO SANTA FE'/(VIA)                       ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16/10/2018</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2168 rows √ó 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Categoria criminologa Sottocategoria Criminologica  Circoscrizione  \\\n",
              "0         Allarme Sociale                        Altro             6.0   \n",
              "1         Allarme Sociale                        Altro             6.0   \n",
              "2         Allarme Sociale                        Altro             4.0   \n",
              "3         Allarme Sociale                        Altro             1.0   \n",
              "4         Allarme Sociale                        Altro             4.0   \n",
              "..                    ...                          ...             ...   \n",
              "990        Qualit√† Urbana      Decoro e degrado urbano             6.0   \n",
              "991        Qualit√† Urbana          Veicoli abbandonati             4.0   \n",
              "992        Qualit√† Urbana          Veicoli abbandonati             4.0   \n",
              "993        Qualit√† Urbana          Veicoli abbandonati             6.0   \n",
              "994        Qualit√† Urbana          Veicoli abbandonati             9.0   \n",
              "\n",
              "                                              Localita Area Verde        Data  \\\n",
              "0    BELMONTE/(VIA)                                ...        NaN  01/02/2018   \n",
              "1    DONATORE DI SANGUE/(PIAZZA DEL)               ...        NaN  12/02/2018   \n",
              "2    CIBRARIO/LUIGI (VIA)                          ...        NaN  26/02/2018   \n",
              "3    ROMA/(VIA)                                    ...        NaN  02/03/2018   \n",
              "4    ZUMAGLIA/(VIA)                                ...        NaN  05/03/2018   \n",
              "..                                                 ...        ...         ...   \n",
              "990  VERCELLI/(CORSO)                              ...        NaN  31/12/2018   \n",
              "991  BOSELLI/PAOLO (VIA)                           ...        NaN  17/09/2018   \n",
              "992  PIFFETTI/PIETRO (VIA)                         ...        NaN  22/09/2018   \n",
              "993  FOSSATA/(VIA)                                 ...        NaN  22/09/2018   \n",
              "994  ROSARIO SANTA FE'/(VIA)                       ...        NaN  16/10/2018   \n",
              "\n",
              "       Ora  \n",
              "0      NaN  \n",
              "1      NaN  \n",
              "2      NaN  \n",
              "3      NaN  \n",
              "4      NaN  \n",
              "..     ...  \n",
              "990  11.08  \n",
              "991    NaN  \n",
              "992  14.01  \n",
              "993   9.55  \n",
              "994    NaN  \n",
              "\n",
              "[2168 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incoming-dairy",
        "outputId": "8d83dde0-b30a-488f-ffe9-71af1b7ab63c"
      },
      "source": [
        "df_weather = pd.read_csv(\"https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/all_weather.csv\")\n",
        "df_weather = df_weather.drop(columns = ['Unnamed: 0'])\n",
        "df_weather.head()"
      ],
      "id": "incoming-dairy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>temp</th>\n",
              "      <th>winds</th>\n",
              "      <th>rainfall_mm</th>\n",
              "      <th>snowfall_mm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-01-01 00:00:00</td>\n",
              "      <td>1.04</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>2.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-01-01 01:00:00</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.590000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>2.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-01-01 02:00:00</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.008</td>\n",
              "      <td>2.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-01-01 03:00:00</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>2.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-01-01 04:00:00</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>2.300000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  time  temp     winds  rainfall_mm  snowfall_mm\n",
              "0  2018-01-01 00:00:00  1.04  0.366667       -0.010     2.600000\n",
              "1  2018-01-01 01:00:00  1.09  0.590000        0.009     2.600000\n",
              "2  2018-01-01 02:00:00  1.05  0.450000        0.008     2.266667\n",
              "3  2018-01-01 03:00:00  0.89  0.400000        0.006     2.266667\n",
              "4  2018-01-01 04:00:00  0.73  0.780000       -0.011     2.300000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "informed-consortium"
      },
      "source": [
        "<br><br>\n",
        "See details in [Appendix](#Weather Data)"
      ],
      "id": "informed-consortium"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "central-velvet",
        "outputId": "eb78b8b5-9a83-4c02-c4a0-94ba2a40d43e"
      },
      "source": [
        "df_holidays = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/holidays.csv')\n",
        "df_holidays"
      ],
      "id": "central-velvet",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Day</th>\n",
              "      <th>Holiday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01-01-2018</td>\n",
              "      <td>monday</td>\n",
              "      <td>New year's Day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>06-01-2018</td>\n",
              "      <td>saturday</td>\n",
              "      <td>La Befana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19-03-2018</td>\n",
              "      <td>monday</td>\n",
              "      <td>Father's day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25-03-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>Palm Sunday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01-04-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>Easter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>02-04-2018</td>\n",
              "      <td>monday</td>\n",
              "      <td>Easter Monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>25-04-2018</td>\n",
              "      <td>wednesday</td>\n",
              "      <td>liberation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>01-05-2018</td>\n",
              "      <td>tuesday</td>\n",
              "      <td>Labour day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>09-05-2018</td>\n",
              "      <td>wednesday</td>\n",
              "      <td>Europe day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13-05-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>mother's day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>20-05-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>Whitsun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>02-06-2018</td>\n",
              "      <td>saturday</td>\n",
              "      <td>republic day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>24-06-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>St. John's Day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>15-07-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>Bernhard II of Baden</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15-08-2018</td>\n",
              "      <td>wednesday</td>\n",
              "      <td>Ferragosto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>21-09-2018</td>\n",
              "      <td>friday</td>\n",
              "      <td>Matthew the Apostle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>28-10-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>DST end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>01-11-2018</td>\n",
              "      <td>thursday</td>\n",
              "      <td>All Saint's Day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>02-12-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>first advent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>08-12-2018</td>\n",
              "      <td>saturday</td>\n",
              "      <td>Immaculate Conception</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>09-12-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>second advent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>16-12-2018</td>\n",
              "      <td>sunday</td>\n",
              "      <td>third advent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>25-12-2018</td>\n",
              "      <td>tuesday</td>\n",
              "      <td>christmas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>26-12-2018</td>\n",
              "      <td>wednesday</td>\n",
              "      <td>St. Stephen's Day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>31-12-2018</td>\n",
              "      <td>monday</td>\n",
              "      <td>New year's Eve</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Date        Day                Holiday\n",
              "0   01-01-2018     monday         New year's Day\n",
              "1   06-01-2018   saturday              La Befana\n",
              "2   19-03-2018     monday           Father's day\n",
              "3   25-03-2018     sunday            Palm Sunday\n",
              "4   01-04-2018     sunday                 Easter\n",
              "5   02-04-2018     monday          Easter Monday\n",
              "6   25-04-2018  wednesday             liberation\n",
              "7   01-05-2018    tuesday             Labour day\n",
              "8   09-05-2018  wednesday             Europe day\n",
              "9   13-05-2018     sunday           mother's day\n",
              "10  20-05-2018     sunday                Whitsun\n",
              "11  02-06-2018   saturday           republic day\n",
              "12  24-06-2018     sunday         St. John's Day\n",
              "13  15-07-2018     sunday   Bernhard II of Baden\n",
              "14  15-08-2018  wednesday             Ferragosto\n",
              "15  21-09-2018     friday    Matthew the Apostle\n",
              "16  28-10-2018     sunday                DST end\n",
              "17  01-11-2018   thursday        All Saint's Day\n",
              "18  02-12-2018     sunday           first advent\n",
              "19  08-12-2018   saturday  Immaculate Conception\n",
              "20  09-12-2018     sunday          second advent\n",
              "21  16-12-2018     sunday           third advent\n",
              "22  25-12-2018    tuesday              christmas\n",
              "23  26-12-2018  wednesday      St. Stephen's Day\n",
              "24  31-12-2018     monday         New year's Eve"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advance-paint",
        "outputId": "76a14b84-8415-40c7-cb11-637adbbc67a5"
      },
      "source": [
        "df_matches = pd.read_csv('GET MATCHES')\n",
        "df_matches"
      ],
      "id": "advance-paint",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'GET MATCHES'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3a23e45d2102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET MATCHES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[0;32m-> 1357\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    640\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GET MATCHES'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "metropolitan-denmark"
      },
      "source": [
        "df_opening_hours = pd.read_csv('GET OPENING HOURS')"
      ],
      "id": "metropolitan-denmark",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "traditional-insulin"
      },
      "source": [
        "### Merging Dataframes"
      ],
      "id": "traditional-insulin"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recovered-bacon"
      },
      "source": [
        "noise1 = df_noise_2018.copy()"
      ],
      "id": "recovered-bacon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "featured-stupid"
      },
      "source": [
        "df_noise_2018['date_hour'] = pd.to_datetime(df_noise_2018['date_hour'])\n",
        "df_noise_2018['date_hour'] = df_noise_2018['date_hour'].dt.strftime(\"%d-%m-%y %H:%M\")"
      ],
      "id": "featured-stupid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "considerable-rebecca"
      },
      "source": [
        "df_noise_2018.head()"
      ],
      "id": "considerable-rebecca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "resistant-coating"
      },
      "source": [
        "df_wifi.rename(columns = {'Time': 'date_time'}, inplace=True)\n",
        "df_wifi.columns"
      ],
      "id": "resistant-coating",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "minimal-somalia"
      },
      "source": [
        "df_wifi['date_time'] = pd.to_datetime(df_wifi['date_time'])\n",
        "df_wifi['date_time'] = df_wifi['date_time'].dt.strftime(\"%d-%m-%y %H:%M\")"
      ],
      "id": "minimal-somalia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "strong-palace"
      },
      "source": [
        "df_weather['time'] = pd.to_datetime(df_weather['time'])\n",
        "df_weather['time'] = df_weather['time'].dt.strftime(\"%d-%m-%y %H:%M\")"
      ],
      "id": "strong-palace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sapphire-cradle"
      },
      "source": [
        "for x, line in enumerate(df_sim_all['data_da']):\n",
        "    df_sim_all['data_da'][x] = line[8:10] + line[4:7] + '-' + line[0:4] +' ' + line[11:16]"
      ],
      "id": "sapphire-cradle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "integral-advisory"
      },
      "source": [
        "df_sim_all.rename(columns= {'data_da' : 'date_time'}, inplace=True)"
      ],
      "id": "integral-advisory",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "worth-connection"
      },
      "source": [
        "df_sim_all"
      ],
      "id": "worth-connection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "empirical-jumping"
      },
      "source": [
        "df_sim_all['date_time'] = pd.to_datetime(df_sim_all['date_time'])\n",
        "df_sim_all['date_time'] = df_sim_all['date_time'].dt.strftime(\"%d-%m-%y %H:%M\")"
      ],
      "id": "empirical-jumping",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "southern-grenada"
      },
      "source": [
        "Merging noise, wifi, sim,weather,... police"
      ],
      "id": "southern-grenada"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "missing-iraqi"
      },
      "source": [
        "df_final = df_noise_2018.merge(df_wifi, left_on= 'date_hour', right_on= 'date_time', how='left')\n",
        "df_final"
      ],
      "id": "missing-iraqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "adaptive-means"
      },
      "source": [
        "df_final_1 = df_final.merge(df_sim_all, left_on= 'date_hour', right_on= 'date_time', how='left')\n",
        "df_final_1"
      ],
      "id": "adaptive-means",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acknowledged-pillow"
      },
      "source": [
        "df_final_2 = df_final_1.merge(df_weather, left_on= 'date_hour', right_on= 'time', how='left')\n",
        "df_final_2"
      ],
      "id": "acknowledged-pillow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "settled-vulnerability"
      },
      "source": [
        "df_final_2.columns"
      ],
      "id": "settled-vulnerability",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biblical-chinese"
      },
      "source": [
        "df_final_3 = df_final_2.drop(columns = ['date_time_x','date_time_y', 'time'] )"
      ],
      "id": "biblical-chinese",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "typical-headline"
      },
      "source": [
        "df_final_3['date_hour'] = pd.to_datetime(df_final_3['date_hour'])\n",
        "df_final_3['date'] = df_final_3['date_hour'].dt.strftime(\"%d-%m-%y\")"
      ],
      "id": "typical-headline",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "decimal-functionality"
      },
      "source": [
        "df_final_3.head()"
      ],
      "id": "decimal-functionality",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prepared-weather"
      },
      "source": [
        "df_finalized = df_final_3.merge(df_holidays, left_on='date', right_on = 'Date', how =\"left\")\n",
        "df_finalized['isHoliday'] = df_finalized['Holiday'].apply(lambda x: 0 if pd.isnull(x)==True else 1)\n",
        "df_finalized.head(30)"
      ],
      "id": "prepared-weather",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bridal-grove"
      },
      "source": [
        "df_finalized = df_finalized.drop(columns= ['Date'])"
      ],
      "id": "bridal-grove",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "large-cancer"
      },
      "source": [
        "df_finalized"
      ],
      "id": "large-cancer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "charitable-personal"
      },
      "source": [
        "df_finalized.info()"
      ],
      "id": "charitable-personal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stable-grocery"
      },
      "source": [
        "df_finalized.to_csv('Noise_weather_wifi_sim_holidays.csv')"
      ],
      "id": "stable-grocery",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "designing-pickup"
      },
      "source": [
        "### Preprocessing Data"
      ],
      "id": "designing-pickup"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "impressive-subject"
      },
      "source": [
        "#noise_2018=pd.read_csv('/content/drive/MyDrive/finals/noise_data/san_salvario_2018.csv',skiprows=[0,1,2,3,4,5,6,7],delimiter=';')\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/Noise_weather_wifi_sim_holidays.csv')\n",
        "#Converting to date time\n",
        "df['date_hour']=pd.to_datetime(df['date_hour'])\n",
        "df=df.drop(columns=['Unnamed: 0','C1','C2','C3','C4','C5,,,,,','date','Day'])\n",
        "df.info()"
      ],
      "id": "impressive-subject",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "small-exclusive"
      },
      "source": [
        "df['date']=df['date_hour'].dt.date\n",
        "df['hour']=df['date_hour'].dt.hour\n",
        "df['day']=df['date_hour'].dt.dayofweek\n",
        "df.head(2)"
      ],
      "id": "small-exclusive",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "demanding-inspiration"
      },
      "source": [
        "set(df['day'])"
      ],
      "id": "demanding-inspiration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "illegal-sheriff"
      },
      "source": [
        "df_noise_2018.head(2)"
      ],
      "id": "illegal-sheriff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "negative-token"
      },
      "source": [
        "noise1"
      ],
      "id": "negative-token",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dominican-examination"
      },
      "source": [
        "noise1['Ora']=pd.to_datetime(noise1['Ora']).dt.hour\n",
        "noise1['Data']=pd.to_datetime(noise1['Data']).dt.date"
      ],
      "id": "dominican-examination",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "electrical-characteristic"
      },
      "source": [
        "#Converting the noise readings into decimal format\n",
        "noise1['C1']=noise1['C1'].apply(lambda x: str(x).replace(',','.'))\n",
        "noise1['C2']=noise1['C2'].apply(lambda x: str(x).replace(',','.'))\n",
        "noise1['C3']=noise1['C3'].apply(lambda x: str(x).replace(',','.'))\n",
        "noise1['C4']=noise1['C4'].apply(lambda x: str(x).replace(',','.'))\n",
        "noise1['C5']=noise1['C5'].apply(lambda x: str(x).replace(',','.'))\n",
        "#Conerting the noise reading to float values\n",
        "noise1['C1']=noise1['C1'].apply(lambda x: float(x))\n",
        "noise1['C2']=noise1['C2'].apply(lambda x: float(x))\n",
        "noise1['C3']=noise1['C3'].apply(lambda x: float(x))\n",
        "noise1['C4']=noise1['C4'].apply(lambda x: float(x))\n",
        "noise1['C5']=noise1['C5'].apply(lambda x: float(x))\n",
        "noise1.head(2)"
      ],
      "id": "electrical-characteristic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geographic-selection"
      },
      "source": [
        "new_df = pd.merge(noise1, df,  how='inner', left_on=['Data','Ora'], right_on = ['date','hour'])\n",
        "new_df.head()"
      ],
      "id": "geographic-selection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "limiting-lender"
      },
      "source": [
        "new_df.columns"
      ],
      "id": "limiting-lender",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raised-possibility"
      },
      "source": [
        "new_df=new_df.drop(columns=['Data','Ora'])"
      ],
      "id": "raised-possibility",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fitting-harassment"
      },
      "source": [
        "#Fillig the null values considering means on hourly basis\n",
        "new_df[\"C1\"] = new_df.groupby([\"hour\",'day'])['C1'].transform(lambda x: x.fillna(round(x.mean(),1)))\n",
        "new_df[\"C2\"] = new_df.groupby([\"hour\",'day'])['C2'].transform(lambda x: x.fillna(round(x.mean(),1)))\n",
        "new_df[\"C3\"] = new_df.groupby([\"hour\",'day'])['C3'].transform(lambda x: x.fillna(round(x.mean(),1)))\n",
        "new_df[\"C4\"] = new_df.groupby([\"hour\",'day'])['C4'].transform(lambda x: x.fillna(round(x.mean(),1)))\n",
        "new_df[\"C5\"] = new_df.groupby([\"hour\",'day'])['C5'].transform(lambda x: x.fillna(round(x.mean(),1)))"
      ],
      "id": "fitting-harassment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sustained-farming"
      },
      "source": [
        "new_df.head(2)"
      ],
      "id": "sustained-farming",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arctic-compression"
      },
      "source": [
        "new_df.isnull().sum()"
      ],
      "id": "arctic-compression",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "parallel-marina"
      },
      "source": [
        "new_df['Log_Avg']=np.log10(((10**(new_df['C1']/10))+(10**(new_df['C2']/10))+(10**(new_df['C3']/10))+(10**(new_df['C4']/10))+(10**(new_df['C5']/10)))/5)*10"
      ],
      "id": "parallel-marina",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fitted-retrieval"
      },
      "source": [
        "new_df.head(2)"
      ],
      "id": "fitted-retrieval",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civilian-fourth"
      },
      "source": [
        "correlation_mat = new_df.corr()\n",
        "sns.heatmap(correlation_mat, annot = True)\n",
        "plt.show()"
      ],
      "id": "civilian-fourth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "former-cement"
      },
      "source": [
        "corr_pairs = correlation_mat.unstack()\n",
        "print(corr_pairs)"
      ],
      "id": "former-cement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "supported-aluminum"
      },
      "source": [
        "sorted_pairs = corr_pairs.sort_values(kind=\"quicksort\")\n",
        "strong_pairs = sorted_pairs[abs(sorted_pairs) > 0.5]\n",
        "strong_pairs"
      ],
      "id": "supported-aluminum",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "listed-wagon"
      },
      "source": [
        "new_df['data_a']"
      ],
      "id": "listed-wagon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tropical-guest"
      },
      "source": [
        "### Data Exploration"
      ],
      "id": "tropical-guest"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "legitimate-interest"
      },
      "source": [
        "df_police[df_police['Ora'].isna()] #many complaints do not have hours associated with them "
      ],
      "id": "legitimate-interest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extra-thong"
      },
      "source": [
        "noise_2018=pd.read_csv('raw_data/noise_data/san_salvario_2018.csv',\n",
        "                       skiprows=8,\n",
        "                       delimiter=';',\n",
        "                      decimal=',',\n",
        "#                       parse_dates=[['Data', 'Ora']],\n",
        "                      )\n",
        "\n",
        "# # workaround for hour concat issue\n",
        "noise_2018['Data'] = pd.to_datetime(noise_2018['Data'], format='%d-%m-%Y', errors='coerce')\n",
        "noise_2018['date_hour'] = noise_2018.apply(lambda x: pd.to_datetime(str(x.Data) + ' ' + str(x.Ora), errors='coerce'), axis=1)\n",
        "noise_2018 = noise_2018.drop(columns=['Data', 'Ora'])\n",
        "\n",
        "\n",
        "noise_2018.info()"
      ],
      "id": "extra-thong",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "killing-instrument"
      },
      "source": [
        "# plot matches with sensor data\n",
        "match = pd.read_csv('raw_data/football/matches_2018.csv', index_col=0).set_index('Date')\n",
        "match['is_match'] = match.is_match.apply(lambda x: x+80 if x == 1 else x)\n",
        "fig = px.line(noise_2018.set_index('date_hour'))\n",
        "fig.add_scatter(x=match.index, \n",
        "                y=match['is_match'], \n",
        "                mode='markers',\n",
        "                name='football match'\n",
        "               )\n"
      ],
      "id": "killing-instrument",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agreed-webster"
      },
      "source": [
        "### Modelling"
      ],
      "id": "agreed-webster"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "varying-constant"
      },
      "source": [
        "#### ARIMA"
      ],
      "id": "varying-constant"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hungarian-bearing"
      },
      "source": [
        "data=pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/Imputed_Data_Final.csv')\n",
        "data=data.drop(columns='Unnamed: 0')\n",
        "data.head(4)"
      ],
      "id": "hungarian-bearing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swedish-silicon"
      },
      "source": [
        "data_i = data.set_index('date_hour')\n",
        "data_i.head(2)"
      ],
      "id": "swedish-silicon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "respiratory-strength"
      },
      "source": [
        "df=data_i['Log_Avg']\n",
        "df.head(2)"
      ],
      "id": "respiratory-strength",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adapted-kernel"
      },
      "source": [
        "df.plot(figsize=(20,5))"
      ],
      "id": "adapted-kernel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "breathing-photographer"
      },
      "source": [
        "additive = seasonal_decompose(df,freq=52, model='additive',extrapolate_trend='freq')"
      ],
      "id": "breathing-photographer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joined-interim"
      },
      "source": [
        "additive_df = pd.concat([additive.seasonal, additive.trend, additive.resid, additive.observed], axis=1)\n",
        "additive_df.columns = ['seasonal', 'trend', 'resid', 'actual_values']\n",
        "additive_df.head()"
      ],
      "id": "joined-interim",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foster-billion"
      },
      "source": [
        "plt.rcParams.update({'figure.figsize': (20,10)})\n",
        "additive.plot().suptitle('Additive Decompose')\n",
        "#The Trend,residuals are interesting, showing periods of high variability."
      ],
      "id": "foster-billion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "attempted-robin"
      },
      "source": [
        "trend = additive.trend\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "result = adfuller(trend.values)\n",
        "print('ADF Statistic: %f' % result[0])\n",
        "print('p-value: %f' % result[1])"
      ],
      "id": "attempted-robin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "proper-alaska"
      },
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "# Original Series\n",
        "fig, axes = plt.subplots(3, 2, sharex=True)\n",
        "axes[0, 0].plot(trend.values); axes[0, 0].set_title('Original Series')\n",
        "plot_acf(trend.values, ax=axes[0, 1]).suptitle('Original Series', fontsize=0)\n",
        "# 1st Differencing\n",
        "diff1 = trend.diff().dropna()\n",
        "axes[1, 0].plot(diff1.values)\n",
        "axes[1, 0].set_title('1st Order Differencing')\n",
        "plot_acf(diff1.values, ax=axes[1, 1]).suptitle('1st Order Differencing', fontsize=0)\n",
        "# 2nd Differencing\n",
        "diff2 = trend.diff().diff().dropna()\n",
        "axes[2, 0].plot(diff2.values)\n",
        "axes[2, 0].set_title('2nd Order Differencing')\n",
        "plot_acf(diff2.values, ax=axes[2, 1]).suptitle('2nd Order Differencing', fontsize=0)"
      ],
      "id": "proper-alaska",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "average-cooling"
      },
      "source": [
        "plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n",
        "size = 100\n",
        "fig, axes = plt.subplots(1, 2, sharex=True)\n",
        "axes[0].plot(diff1.values[:size])\n",
        "axes[0].set_title('1st Order Differencing')\n",
        "axes[1].set(ylim=(0,5))\n",
        "plot_pacf(diff1.values[:size], lags=50, ax=axes[1]).suptitle('1st Order Differencing', fontsize=0)"
      ],
      "id": "average-cooling",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "associate-humidity"
      },
      "source": [
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "train = trend[:3000]\n",
        "test  = trend[3000:]\n",
        "# order = (p=1, d=1, q=1)\n",
        "model = ARIMA(train, order=(1, 1, 1))  \n",
        "model = model.fit(disp=0)  \n",
        "print(model.summary())"
      ],
      "id": "associate-humidity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "overhead-depth"
      },
      "source": [
        "# Plot residual errors\n",
        "residuals = pd.DataFrame(model.resid)\n",
        "fig, ax = plt.subplots(1,2)\n",
        "residuals.plot(title=\"Residuals\", ax=ax[0])\n",
        "residuals.plot(kind='kde', title='Density', ax=ax[1])"
      ],
      "id": "overhead-depth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "included-coverage"
      },
      "source": [
        "fc, se, conf = model.forecast(14311, alpha=0.05)\n",
        "# Make as pandas series\n",
        "fc_series = pd.Series(fc, index=test.index)\n",
        "lower_series = pd.Series(conf[:, 0], index=test.index)\n",
        "upper_series = pd.Series(conf[:, 1], index=test.index)\n",
        "# Plot\n",
        "plt.figure(figsize=(12,5), dpi=100)\n",
        "plt.plot(train, label='training')\n",
        "plt.plot(test, label='actual')\n",
        "plt.plot(fc_series, label='forecast')\n",
        "plt.fill_between(lower_series.index, lower_series, upper_series, \n",
        "                 color='k', alpha=.15)\n",
        "plt.title('Forecast vs Actuals')\n",
        "plt.legend(loc='upper left', fontsize=8)"
      ],
      "id": "included-coverage",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adult-jason"
      },
      "source": [
        "#### Moving Average"
      ],
      "id": "adult-jason"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "working-alloy"
      },
      "source": [
        "df.head(2)"
      ],
      "id": "working-alloy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "average-fitting"
      },
      "source": [
        "Moving Average Smoothing is a technique applied to time series to remove the fine-grained variation between time steps. The hope of smoothing is to remove noise and better expose the signal of the underlying causal processes."
      ],
      "id": "average-fitting"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "electrical-peter"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,6)\n",
        "df.plot()\n",
        "pyplot.show()"
      ],
      "id": "electrical-peter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "royal-attachment"
      },
      "source": [
        "# Tail-rolling average transform\n",
        "rolling = df.rolling(window=3)\n",
        "rolling_mean = rolling.mean()\n",
        "rolling_mean.dropna(inplace= True)\n",
        "print(rolling_mean.head())\n",
        "# plot original and transformed dataset\n",
        "df.plot()\n",
        "rolling_mean.plot(color='lightgreen')\n",
        "pyplot.show()"
      ],
      "id": "royal-attachment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "valuable-madonna"
      },
      "source": [
        "from pandas import read_csv\n",
        "from numpy import mean\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot\n",
        "# prepare situation\n",
        "X = df.values\n",
        "window = 3\n",
        "history = [X[i] for i in range(window)]\n",
        "test = [X[i] for i in range(window, len(X))]\n",
        "predictions = list()\n",
        "# walk forward over time steps in test\n",
        "for t in range(len(test)):\n",
        "\tlength = len(history)\n",
        "\tyhat = mean([history[i] for i in range(length-window,length)])\n",
        "\tobs = test[t]\n",
        "\tpredictions.append(yhat)\n",
        "\thistory.append(obs)\n",
        "\t#print('predicted=%f, expected=%f' % (yhat, obs))\n",
        "error = mean_squared_error(test, predictions)\n",
        "print('Test MSE: %.3f' % error)\n",
        "# plot\n",
        "pyplot.plot(test)\n",
        "pyplot.plot(predictions, color='lightcoral')\n",
        "pyplot.show()\n",
        "# zoom plot\n",
        "pyplot.plot(test[0:100])\n",
        "pyplot.plot(predictions[0:100], color='lightcoral')\n",
        "pyplot.show()"
      ],
      "id": "valuable-madonna",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simplified-gateway"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "### Scalability and Impact\n",
        "Tell us how applicable and scalable your solution is if you were to implement it in a city. Identify possible limitations and measure the potential social impact of your solution."
      ],
      "id": "simplified-gateway"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heated-thread"
      },
      "source": [
        "### Future Work\n",
        "Now picture the following scenario: imagine you could have access to any type of data that could help you solve this challenge even better. What would that data be and how would it improve your solution? üöÄ"
      ],
      "id": "heated-thread"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "british-contamination"
      },
      "source": [
        "# Appendix "
      ],
      "id": "british-contamination"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "eligible-transformation"
      },
      "source": [
        "## Weather Data"
      ],
      "id": "eligible-transformation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "potential-affiliation"
      },
      "source": [
        "### Data"
      ],
      "id": "potential-affiliation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "cognitive-width"
      },
      "source": [
        "weather = pd.read_csv('raw_data/weather/weather_1.csv',\n",
        "#                       nrows=1000, #rm later\n",
        "                      sep=';',\n",
        "#                       decimal=',',\n",
        "                      skiprows=4,\n",
        "#                       parse_dates=[[0, 1]],\n",
        "#                       dayfirst=True,\n",
        "                      header=0,\n",
        "                      names=['date', 'hour', 'rainfall_mm', 'snowfall_mm'],\n",
        "                     )\n",
        "\n",
        "# workaround for hour concat issue\n",
        "weather['date'] = pd.to_datetime(weather['date'], format='%d-%m-%Y', errors='coerce')\n",
        "weather['date_hour'] = weather.apply(lambda x: pd.to_datetime(str(x.date) + ' ' + str(x.hour), errors='coerce'), axis=1)\n",
        "\n",
        "# workaround for decimal issue\n",
        "weather['rainfall_mm'] = weather.rainfall_mm.apply(lambda x: str(x).replace(',','.'))\n",
        "weather['snowfall_mm'] = weather.snowfall_mm.apply(lambda x: str(x).replace(',','.'))"
      ],
      "id": "cognitive-width",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "reasonable-darkness"
      },
      "source": [
        "weather2 = pd.read_csv('raw_data/weather/weather_2.csv', \n",
        "                 sep=';', \n",
        "                 skiprows=4, \n",
        "                 header=0, \n",
        "#                  decimal=',',\n",
        "#                 converters={2:lambda x: x.replace(',', '.')},\n",
        "#                 parse_dates=[[0, 1]],\n",
        "                names=['date', 'hour', 'winds'],\n",
        "                na_values={2:'',\n",
        "                            3:''},\n",
        "                dayfirst=True,\n",
        "                )\n",
        "# workaround for hour concat issue\n",
        "weather2['date'] = pd.to_datetime(weather2['date'], format='%d-%m-%Y', errors='coerce')\n",
        "weather2['date_hour'] = weather2.apply(lambda x: pd.to_datetime(str(x.date) + ' ' + str(x.hour), errors='coerce'), axis=1)\n",
        "\n",
        "weather2['winds'] = weather2.winds.apply(lambda x: str(x).replace(',','.'))"
      ],
      "id": "reasonable-darkness",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "technological-meaning"
      },
      "source": [
        "# weather['date_hour'] = pd.to_datetime(weather['date_hour'], errors='coerce')\n",
        "weather_1 = weather.dropna(subset=['date_hour'])\n",
        "\n",
        "# weather2['date_hour'] = pd.to_datetime(weather2['date_hour'], errors='coerce')\n",
        "weather_2 = weather2.dropna(subset=['date_hour'])"
      ],
      "id": "technological-meaning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "italian-maldives"
      },
      "source": [
        "merged_weather = weather_2.merge(weather_1,\n",
        "                                right_on='date_hour',\n",
        "                                left_on='date_hour',\n",
        "                                )"
      ],
      "id": "italian-maldives",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "marked-morrison"
      },
      "source": [
        "merged_weather.sort_values(by='date_hour').tail()"
      ],
      "id": "marked-morrison",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "expected-broadway"
      },
      "source": [
        "merged_weather['hourly_date'] = merged_weather.date_hour.apply(lambda x: x.floor('h'))"
      ],
      "id": "expected-broadway",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "scientific-behavior"
      },
      "source": [
        "merged_weather = merged_weather.astype({'winds': float,\n",
        "                      'rainfall_mm':float,\n",
        "                      'snowfall_mm':float})"
      ],
      "id": "scientific-behavior",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "passive-sphere"
      },
      "source": [
        "hourly_weather = merged_weather.groupby('hourly_date').mean()"
      ],
      "id": "passive-sphere",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "perceived-nerve"
      },
      "source": [
        "hourly_weather.info()"
      ],
      "id": "perceived-nerve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "approximate-doctor"
      },
      "source": [
        "hourly_weather.to_csv('hourly_weather.csv')"
      ],
      "id": "approximate-doctor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "abstract-luxury"
      },
      "source": [
        "hourly_weather.head()"
      ],
      "id": "abstract-luxury",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "alert-effects"
      },
      "source": [
        "### Open Weather Map"
      ],
      "id": "alert-effects"
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "distinct-repository"
      },
      "source": [
        "# API KEY\n",
        "load_dotenv(find_dotenv())\n",
        "OWM_API = os.environ.get(\"OWM_API\")"
      ],
      "id": "distinct-repository",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "fluid-height"
      },
      "source": [
        "# init time range\n",
        "range_2019 = pd.DataFrame(pd.date_range('2016-06-01', '2021-06-12', freq='h'), columns=['hour'])\n",
        "range_2019.tail().hour"
      ],
      "id": "fluid-height",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "human-soldier"
      },
      "source": [
        "req = 'http://history.openweathermap.org/data/2.5/history/wdl'\n",
        "start = range_2019.hour.min().value\n",
        "inter = range_2019.hour.max().value\n",
        "end = range_2019.hour.max().value\n",
        "# tail1 = tail.min().value\n",
        "# tail2 = tail.max().value\n",
        "params = {\n",
        "    'id':'3165524', # ID of Turin\n",
        "    'type':'hour',\n",
        "    'start':str(start)[:10], # unix time\n",
        "    'end':str(end)[:10],\n",
        "    'appid': OWM_API\n",
        "}\n",
        "\n",
        "r = requests.get(req, params=params)\n",
        "\n",
        "\n",
        "# with open('data/weather.txt', 'w') as outfile:\n",
        "#     json.dump(r.json(), outfile)\n",
        "    \n",
        "weather = r.json()\n",
        "lst = weather.get('list')\n",
        "dct = {x.get('dt'):x.get('weather')[0].get('main') for x in lst}\n",
        "weather_df = pd.DataFrame.from_dict(dct, \n",
        "                                    orient='index', \n",
        "                                    columns=['weather']).reset_index().rename(columns={'index':'time'})\n",
        "weather_df['rain'] = weather_df.weather == 'Rain'"
      ],
      "id": "human-soldier",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "light-tractor"
      },
      "source": [
        "lst = weather.get('list')\n",
        "dct = {x.get('dt'):x.get('main').get('temp') for x in lst}"
      ],
      "id": "light-tractor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "indoor-kenya"
      },
      "source": [
        "weather_df = pd.DataFrame.from_dict(dct, \n",
        "                                    orient='index', \n",
        "                                    columns=['temp']).reset_index().rename(columns={'index':'time'})\n",
        "weather_df['temp'] = weather_df.temp-273.15\n",
        "weather_df['time'] = pd.to_datetime(weather_df.time, unit='s')"
      ],
      "id": "indoor-kenya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "respective-quebec"
      },
      "source": [
        "weather_df.info()"
      ],
      "id": "respective-quebec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "warming-candle"
      },
      "source": [
        "merge_all = weather_df.merge(hourly_weather, left_on='time', right_index=True)"
      ],
      "id": "warming-candle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "worldwide-biology"
      },
      "source": [
        "merge_all.to_csv('all_weather.csv')"
      ],
      "id": "worldwide-biology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "competitive-future"
      },
      "source": [
        "merge_all"
      ],
      "id": "competitive-future",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "revised-antenna"
      },
      "source": [
        "## Matches Data"
      ],
      "id": "revised-antenna"
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "dedicated-europe"
      },
      "source": [
        "# API KEY\n",
        "load_dotenv(find_dotenv())\n",
        "FOOTBALL = os.environ.get(\"FOOTBALL\")"
      ],
      "id": "dedicated-europe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "plastic-routine"
      },
      "source": [
        "# headers = {'X-Auth-Token': FOOTBALL}\n",
        "# url = 'https://api.football-data.org/v2/matches'\n",
        "# params = {'dateFrom': '2018-04-14',\n",
        "#          'dateTo': '2018-04-16'}\n",
        "# r = requests.get(url, headers=headers, params=params)\n",
        "# r.json()"
      ],
      "id": "plastic-routine",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "peripheral-charles"
      },
      "source": [
        "root = 'raw_data/football/'\n",
        "dfs = []\n",
        "for i in os.listdir(root):\n",
        "    if '.csv' in i:\n",
        "        df = pd.read_csv(root+i)\n",
        "        dfs.append(df)"
      ],
      "id": "peripheral-charles",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "assisted-airport"
      },
      "source": [
        "# filter all by juve\n",
        "juve1 = dfs[0][(dfs[0]['HomeTeam'] == 'Juventus') \\\n",
        "              | (dfs[0]['AwayTeam'] == 'Juventus')]['Date']\n",
        "juve1 = pd.to_datetime(juve1, format='%d/%m/%Y')\n",
        "\n",
        "juve2 = dfs[1][(dfs[1]['Home Team'] == 'Juventus') \\\n",
        "               | (dfs[1]['Away Team'] == 'Juventus')]['Date']\n",
        "juve2 = pd.to_datetime(juve2.apply(lambda x: x[:10]), format=\"%d/%m/%Y\")\n",
        "\n",
        "juve3 = dfs[2][(dfs[2]['HomeTeam'] == 'Juventus') \\\n",
        "              | (dfs[2]['AwayTeam'] == 'Juventus')]['Date']\n",
        "juve3 = pd.to_datetime(juve3, format='%d/%m/%y')\n",
        "\n",
        "juve4 = dfs[3][(dfs[3]['Home Team'] == 'Juventus') \\\n",
        "               | (dfs[3]['Away Team'] == 'Juventus')]['Date']\n",
        "juve4 = pd.to_datetime(juve4.apply(lambda x: x[:10]), format=\"%d/%m/%Y\")"
      ],
      "id": "assisted-airport",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "pressed-legislature"
      },
      "source": [
        "# concat all dates\n",
        "all_concat = pd.DataFrame(pd.concat([juve1, juve2, juve3, juve4]))\n",
        "# all_concat['Date'] = pd.to_datetime(all_concat.Date)\n",
        "all_concat['is_match'] = 1"
      ],
      "id": "pressed-legislature",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "irish-booking"
      },
      "source": [
        "all_concat.sort_values(by='Date')"
      ],
      "id": "irish-booking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "intelligent-wyoming"
      },
      "source": [
        "# get all 2018 matches\n",
        "all_concat_2018 = all_concat[(all_concat.Date > '01-01-2018') \\\n",
        "                            & (all_concat.Date < '2018-12-31')]"
      ],
      "id": "intelligent-wyoming",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "accredited-cartoon"
      },
      "source": [
        "# put in 2018 time series\n",
        "r = pd.date_range('2018-01-01', '2018-12-31', freq='h')\n",
        "matches = all_concat_2018.set_index('Date').reindex(r).rename_axis('Date').reset_index()"
      ],
      "id": "accredited-cartoon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "later-programming"
      },
      "source": [
        "matches.head()"
      ],
      "id": "later-programming",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "spare-london"
      },
      "source": [
        "matches.to_csv('raw_data/football/matches_2018.csv')"
      ],
      "id": "spare-london",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "straight-uzbekistan"
      },
      "source": [
        "## Opening Hours Data"
      ],
      "id": "straight-uzbekistan"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "adopted-glenn"
      },
      "source": [
        "This part is a bit messy, so we will explain: \n",
        "We used the ```nearbysearch``` [Link](https://developers.google.com/maps/documentation/places/web-service/search#PlaceSearchRequests) to get all the ```bars``` and ```restaurants``` business hours. \n",
        "\n",
        "Then we fetch the unique id ```reference``` from the list of businesses and run it through the ```place_details``` API [Link](https://developers.google.com/maps/documentation/places/web-service/details)\n",
        "\n",
        "From there we extract all the ```open``` (time-)elements and ```close``` (time-)elements and stack them in a dataframe divided by days of the week (0-6). \n",
        "\n",
        "In the end we merge the findings with an empty time series of 2018 with an 'hourly' sequence. \n"
      ],
      "id": "adopted-glenn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "understanding-opera"
      },
      "source": [
        "# API KEY\n",
        "load_dotenv(find_dotenv())\n",
        "GOOGLE = os.environ.get(\"GOOGLE\")"
      ],
      "id": "understanding-opera",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "varying-engineer"
      },
      "source": [
        "# first find all bars\n",
        "\n",
        "url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json'\n",
        "params = {            \n",
        "            'location':'45.05917,7.67899', #sensor\n",
        "            'radius':'200',\n",
        "            'type':'restaurant',\n",
        "            'key':GOOGLE,\n",
        "            'next_page_token':'Aap_uED24ODLIlOhPdAHG7xFrCg_OrsQ_jAruvTm3QSG4Qbnp5Q85Aa4K7ar-QgnGI7Xnl1epc9YIEj17piMfVpFUxQysBwi8XTzdWbtl6IBGKTKQwV_kxhaAUWr8JG6XVo-BVKHd8NJUwiTP-_uQvkKxc5vLZ4-v6T8ZBuS42zw5DE1L2KgNPCbm86EsPhPYOj8L1MXTRdEm_GhmQSdOt8nDxG4gKkbxiXvmHNTmuBLavqN-VrbpkRBBoVZz_t2P53_ShPgndMEwlt55EYlZHCYK2gHymy9WJjMjKn3VzS6CfcTQJ-TjgsxsrRjSqNXV4T5i2qusSJ__gsam11RBY8XRADB31i-ec_wYCh1529gNKKy9tdQbidVaQjAI72wQ-7yzTZXGzxpz8ob_DHkdVdyJLxijWoHqsXY7oQM-W3Db0u08SHwaooMyb3Da9Ij'\n",
        "         } \n",
        "\n",
        "r = requests.get(url, params=params)\n",
        "r.json()"
      ],
      "id": "varying-engineer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "christian-coaching"
      },
      "source": [
        "results = r.json().get('results')\n",
        "results2 = r.json().get('results')\n",
        "results3 = r.json().get('results')\n",
        "results4 = r.json().get('results')\n",
        "results5 = r.json().get('results')\n",
        "results6 = r.json().get('results')"
      ],
      "id": "christian-coaching",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "beginning-decrease"
      },
      "source": [
        "bars = results + results2 + results3 + results4 + results5 + results6\n",
        "restaurants = results + results2 + results3 + results4 + results5 + results6\n",
        "len(restaurants)"
      ],
      "id": "beginning-decrease",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "known-consciousness"
      },
      "source": [
        "# get specific opening hrs from fetched bars/restaurants\n",
        "url = 'https://maps.googleapis.com/maps/api/place/details/json'\n",
        "params = {\n",
        "    'key':GOOGLE,\n",
        "    'fields':'opening_hours'\n",
        "         }\n",
        "opening_hrs = []\n",
        "for bar in restaurants:\n",
        "    reference = bar.get('reference')\n",
        "    params['place_id'] = reference\n",
        "    r = requests.get(url, params=params)\n",
        "    opening_hrs.append(r)"
      ],
      "id": "known-consciousness",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "champion-people"
      },
      "source": [
        "contents_hrs = [r.json() for r in opening_hrs]\n",
        "periods = []\n",
        "for x in contents_hrs:\n",
        "    try:\n",
        "        hr = x.get('result').get('opening_hours').get('periods')\n",
        "        periods.append(hr)\n",
        "    except:\n",
        "        pass"
      ],
      "id": "champion-people",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "exact-scroll"
      },
      "source": [
        "# remove 24h open bars\n",
        "new = [x for x in periods if len(x) > 1]"
      ],
      "id": "exact-scroll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "detailed-agent"
      },
      "source": [
        "closing = []\n",
        "for x in new:\n",
        "    for i in x:\n",
        "        _close = i.get('close')\n",
        "        closing.append(_close)\n",
        "opening = []\n",
        "for x in new:\n",
        "    for i in x:\n",
        "        _open = i.get('open')\n",
        "        opening.append(_open)"
      ],
      "id": "detailed-agent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "impressed-bandwidth"
      },
      "source": [
        "opening_times_rest = pd.DataFrame(opening)\n",
        "closing_times_rest = pd.DataFrame(closing)\n",
        "closing_times_rest['time'] = pd.to_datetime(closing_times_rest['time'], format='%H%M')\n",
        "opening_times_rest['time'] = pd.to_datetime(opening_times_rest['time'], format='%H%M')\n",
        "closing_times_rest['day'] = closing_times_rest.day.apply(lambda x: x-1 if x != 0 else 6)\n",
        "opening_times_rest['day'] = opening_times_rest.day.apply(lambda x: x-1 if x != 0 else 6)"
      ],
      "id": "impressed-bandwidth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "introductory-outreach"
      },
      "source": [
        "# create unique day_hr identifier\n",
        "closing_times_rest['day_time'] = closing_times_rest.apply(lambda x: str(x.day) + '_' + str(x.time.hour), axis=1)\n",
        "opening_times_rest['day_time'] = opening_times_rest.apply(lambda x: str(x.day) + '_' + str(x.time.hour), axis=1)"
      ],
      "id": "introductory-outreach",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "seventh-practitioner"
      },
      "source": [
        "# put results in dataframe\n",
        "opening_times = pd.DataFrame(opening)\n",
        "closing_times = pd.DataFrame(closing)\n",
        "closing_times['time'] = pd.to_datetime(closing_times['time'], format='%H%M')\n",
        "opening_times['time'] = pd.to_datetime(opening_times['time'], format='%H%M')\n",
        "closing_times['day'] = closing_times.day.apply(lambda x: x-1 if x != 0 else 6)\n",
        "opening_times['day'] = opening_times.day.apply(lambda x: x-1 if x != 0 else 6)"
      ],
      "id": "seventh-practitioner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "wireless-silly"
      },
      "source": [
        "# create unique day_hr identifier\n",
        "closing_times['day_time'] = closing_times.apply(lambda x: str(x.day) + '_' + str(x.time.hour), axis=1)\n",
        "opening_times['day_time'] = opening_times.apply(lambda x: str(x.day) + '_' + str(x.time.hour), axis=1)"
      ],
      "id": "wireless-silly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "unable-sarah"
      },
      "source": [
        "closing_all = pd.concat([closing_times_rest, closing_times])\n",
        "opening_all = pd.concat([opening_times_rest, opening_times])"
      ],
      "id": "unable-sarah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "removable-arrest"
      },
      "source": [
        "# count all apperances of openings and closings per weekday\n",
        "agg_close = closing_all.groupby('day_time').agg({'day':'count'}).rename(columns={'day':'count_close'})\n",
        "agg_open = opening_all.groupby('day_time').agg({'day':'count'}).rename(columns={'day':'count_open'})\n",
        "agg_joint = agg_close.join(agg_open, how='outer')"
      ],
      "id": "removable-arrest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "homeless-wrist"
      },
      "source": [
        "# init range 2018\n",
        "range_2018 = pd.DataFrame(pd.date_range('2018-01-01', '2018-12-31', freq='h'), columns=['hour'])\n",
        "range_2018['day_time'] =  range_2018.apply(lambda x: str(x.hour.weekday()) + '_' + str(x.hour.hour), axis=1)"
      ],
      "id": "homeless-wrist",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "every-techno"
      },
      "source": [
        "# join both\n",
        "\n",
        "opening_count_2018 = range_2018.merge(agg_joint, \n",
        "                                    on='day_time',\n",
        "                                    how='left').drop(columns='day_time')"
      ],
      "id": "every-techno",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "threatened-knife"
      },
      "source": [
        "opening_times"
      ],
      "id": "threatened-knife",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "welcome-henry"
      },
      "source": [
        "opening_count_2018.sort_values(by='count_open')"
      ],
      "id": "welcome-henry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "qualified-drawing"
      },
      "source": [
        "opening_count_2018.to_csv('raw_data/opening_count_2018.csv')"
      ],
      "id": "qualified-drawing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No9kMhk9EJwP"
      },
      "source": [
        ""
      ],
      "id": "No9kMhk9EJwP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stsg_9K2EJsn"
      },
      "source": [
        ""
      ],
      "id": "stsg_9K2EJsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPj_thXYEJqJ"
      },
      "source": [
        ""
      ],
      "id": "XPj_thXYEJqJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYWIw5YREJmK"
      },
      "source": [
        ""
      ],
      "id": "CYWIw5YREJmK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC3Oa3f8EKIA"
      },
      "source": [
        "# Modelling "
      ],
      "id": "QC3Oa3f8EKIA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzV8NHNzFP0_"
      },
      "source": [
        "##Preprocessing the final dataframe"
      ],
      "id": "hzV8NHNzFP0_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKsXqfBzEJjz"
      },
      "source": [
        "df_merged = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/Noise_weather_wifi_sim_holidays_opencount_complaints_logvalues.csv')\n",
        "df_merged = df_merged.drop(columns=['Unnamed: 0'])"
      ],
      "id": "PKsXqfBzEJjz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGsP999EU1o"
      },
      "source": [
        "For now let us consider log_avg (logarithmic average of C1, C2, C3, C4, C5), temp, rainfall, snowfall, isholiday, complaints_no, count_close, count_open"
      ],
      "id": "1KGsP999EU1o"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5VNKOoAERkd"
      },
      "source": [
        "df_reg_1 = df_merged[['date_hour', 'temp', 'winds', 'rainfall_mm', 'snowfall_mm', 'isHoliday', 'date', 'hour', 'day', 'Log_Avg', 'Complaints_no', 'count_close']]\n",
        "df_reg_1 = df_reg_1.fillna(value={'count_close' : 0, 'winds' : 0, 'snowfall_mm':0 , 'rainfall_mm' : 0})\n",
        "df_reg_1['date_hour'] = pd.to_datetime(df_reg_1['date_hour'])\n",
        "df_reg_1 =  df_reg_1.set_index('date_hour')"
      ],
      "id": "Z5VNKOoAERkd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unTP5eSCEYv-"
      },
      "source": [
        "cor = df_reg_1.corr()\n",
        "cor['Log_Avg'].sort_values()"
      ],
      "id": "unTP5eSCEYv-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2uC9LKNEfvQ"
      },
      "source": [
        "df_close = df_reg_1.copy()"
      ],
      "id": "a2uC9LKNEfvQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p_e7VDHErMM"
      },
      "source": [
        "## Getting Log_Avg values of previous times"
      ],
      "id": "-p_e7VDHErMM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0LBqf6OEqEO"
      },
      "source": [
        "def create_regressor_attributes(df, attribute, list_of_prev_t_instants) :\n",
        "    \n",
        "    list_of_prev_t_instants.sort()\n",
        "    start = list_of_prev_t_instants[-1] \n",
        "    end = len(df)\n",
        "    df['datetime'] = df.index\n",
        "    df.reset_index(drop=True)\n",
        "\n",
        "    df_copy = df[start:end]\n",
        "    df_copy.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    for attribute in attribute :\n",
        "            foobar = pd.DataFrame()\n",
        "\n",
        "            for prev_t in list_of_prev_t_instants :\n",
        "                new_col = pd.DataFrame(df[attribute].iloc[(start - prev_t) : (end - prev_t)])\n",
        "                new_col.reset_index(drop=True, inplace=True)\n",
        "                new_col.rename(columns={attribute : '{}_(t-{})'.format(attribute, prev_t)}, inplace=True)\n",
        "                foobar = pd.concat([foobar, new_col], sort=False, axis=1)\n",
        "\n",
        "            df_copy = pd.concat([df_copy, foobar], sort=False, axis=1)\n",
        "            \n",
        "    df_copy.set_index(['datetime'], drop=True, inplace=True)\n",
        "    return df_copy"
      ],
      "id": "d0LBqf6OEqEO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMnL7sPxE21U"
      },
      "source": [
        "list_of_attributes = ['Log_Avg']\n",
        "\n",
        "list_of_prev_t_instants = []\n",
        "for i in range(24,361,24): #we can change this list as list of times with most impact\n",
        "    list_of_prev_t_instants.append(i)\n",
        "\n",
        "list_of_prev_t_instants"
      ],
      "id": "bMnL7sPxE21U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxG_eYydFFNO"
      },
      "source": [
        "df_new = create_regressor_attributes(df_close, list_of_attributes, list_of_prev_t_instants)\n",
        "df_new.head()"
      ],
      "id": "ZxG_eYydFFNO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYnpFFE4FIjA"
      },
      "source": [
        "df_new.corr()['Log_Avg'].sort_values()"
      ],
      "id": "bYnpFFE4FIjA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8K_w6xTFeoO"
      },
      "source": [
        "Probably we should do some more preprocessing. Currently I am sticking to just the modelling. Later on we can change it accordingly"
      ],
      "id": "t8K_w6xTFeoO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmfC_8fEFJTb"
      },
      "source": [
        "df_new_1 = df_new[['Log_Avg', 'Log_Avg_(t-24)',\n",
        "       'Log_Avg_(t-48)', 'Log_Avg_(t-72)', 'Log_Avg_(t-96)', 'Log_Avg_(t-120)',\n",
        "       'Log_Avg_(t-144)', 'Log_Avg_(t-168)', 'Log_Avg_(t-192)','Log_Avg_(t-216)', 'Log_Avg_(t-240)',\n",
        "       'temp', 'winds', 'rainfall_mm', 'snowfall_mm', 'isHoliday', \n",
        "       'hour', 'day',  'Complaints_no', 'count_close']]"
      ],
      "id": "TmfC_8fEFJTb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kafyEnYTFlMq"
      },
      "source": [
        "##Spliting the dataset into train, validation and test sets"
      ],
      "id": "kafyEnYTFlMq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwhv84W5FsX0"
      },
      "source": [
        "test_set_size = 0.05\n",
        "valid_set_size= 0.05\n",
        "\n",
        "df_copy = df_new_1.reset_index(drop=True)\n",
        "\n",
        "df_test = df_copy.iloc[ int(np.floor(len(df_copy)*(1-test_set_size))) : ]\n",
        "df_train_plus_valid = df_copy.iloc[ : int(np.floor(len(df_copy)*(1-test_set_size))) ]\n",
        "\n",
        "df_train = df_train_plus_valid.iloc[ : int(np.floor(len(df_train_plus_valid)*(1-valid_set_size))) ]\n",
        "df_valid = df_train_plus_valid.iloc[ int(np.floor(len(df_train_plus_valid)*(1-valid_set_size))) : ]\n",
        "\n",
        "\n",
        "X_train, y_train = df_train.iloc[:, 1:], df_train.iloc[:, 0]\n",
        "X_valid, y_valid = df_valid.iloc[:, 1:], df_valid.iloc[:, 0]\n",
        "X_test, y_test = df_test.iloc[:, 1:], df_test.iloc[:, 0]\n",
        "\n",
        "print('Shape of training inputs, training target:', X_train.shape, y_train.shape)\n",
        "print('Shape of validation inputs, validation target:', X_valid.shape, y_valid.shape)\n",
        "print('Shape of test inputs, test target:', X_test.shape, y_test.shape)"
      ],
      "id": "rwhv84W5FsX0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko82GCOGFuY_"
      },
      "source": [
        "## Implementing a minmaxscaler ## we can skip this"
      ],
      "id": "ko82GCOGFuY_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2fHLfvKFseY"
      },
      "source": [
        "Target_scaler = MinMaxScaler(feature_range=(0.01, 0.99)) \n",
        "Feature_scaler = MinMaxScaler(feature_range=(0.01, 0.99))\n",
        "\n",
        "X_train_scaled = Feature_scaler.fit_transform(np.array(X_train))\n",
        "X_valid_scaled = Feature_scaler.fit_transform(np.array(X_valid))\n",
        "X_test_scaled = Feature_scaler.fit_transform(np.array(X_test))\n",
        "\n",
        "y_train_scaled = Target_scaler.fit_transform(np.array(y_train).reshape(-1,1))\n",
        "y_valid_scaled = Target_scaler.fit_transform(np.array(y_valid).reshape(-1,1))\n",
        "y_test_scaled = Target_scaler.fit_transform(np.array(y_test).reshape(-1,1))"
      ],
      "id": "q2fHLfvKFseY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w-9SITIF-NV"
      },
      "source": [
        "## Modelling"
      ],
      "id": "5w-9SITIF-NV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSwRq3NwGH2-"
      },
      "source": [
        "currently adding all the model, based on the score we can use the top model and move the rest to appendix or delete them"
      ],
      "id": "xSwRq3NwGH2-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPxCqhPXGE5B"
      },
      "source": [
        "### Linear Regression"
      ],
      "id": "UPxCqhPXGE5B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RD60asFGbdD"
      },
      "source": [
        "Lin_reg = LinearRegression()\n",
        "Lin_reg.fit(X_train_scaled, y_train_scaled)\n",
        "y_pred = Lin_reg.predict(X_test_scaled)\n",
        "y_pred_rescaled = Target_scaler.inverse_transform(y_pred)\n",
        "\n",
        "y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)\n",
        "score = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "print('R-squared score for the test set: ', round(score,4))\n"
      ],
      "id": "2RD60asFGbdD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN7IOr8VGebY"
      },
      "source": [
        "### Ridge Regression"
      ],
      "id": "sN7IOr8VGebY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nST3_HQFGhQ3"
      },
      "source": [
        "ridge = Ridge(alpha=0.5)\n",
        "ridge.fit(X_train_scaled, y_train_scaled)\n",
        "y_pred = ridge.predict(X_test_scaled)\n",
        "y_pred_rescaled = Target_scaler.inverse_transform(y_pred)\n",
        "\n",
        "y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)\n",
        "score = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "print('R-squared score for the test set: ', round(score,4))\n"
      ],
      "id": "nST3_HQFGhQ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AcrPXrcGj94"
      },
      "source": [
        "### Lasso Regression"
      ],
      "id": "_AcrPXrcGj94"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4vCE08tGpBZ"
      },
      "source": [
        "\n",
        "Lasso = Lasso(alpha=0.2, fit_intercept=True, normalize=False, precompute=False, max_iter=1000,\n",
        "              tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
        "Lasso.fit(X_train_scaled, y_train_scaled)\n",
        "y_pred = Lasso.predict(X_test_scaled)\n",
        "y_pred_rescaled = Target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)\n",
        "score = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "print('R-squared score for the test set: ', round(score,4))\n"
      ],
      "id": "-4vCE08tGpBZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tvHMxatGsWr"
      },
      "source": [
        "### Decision Tree regression"
      ],
      "id": "6tvHMxatGsWr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCOFoeAnGrq3"
      },
      "source": [
        "tree_model = DecisionTreeRegressor()\n",
        "tree_model.fit(X_train_scaled, y_train_scaled)\n",
        "y_pred = tree_model.predict(X_test_scaled)\n",
        "y_pred_rescaled = Target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)\n",
        "score = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "print('R-squared score for the test set: ', round(score,4))"
      ],
      "id": "lCOFoeAnGrq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Z6mLjyGw6a"
      },
      "source": [
        "### Random Forest Regressor"
      ],
      "id": "Y5Z6mLjyGw6a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI4PsdRxG0w_"
      },
      "source": [
        "Rfr = rfr(n_estimators = 100, criterion = 'mse',\n",
        "                              random_state = 1,\n",
        "                              n_jobs = -1)\n",
        "Rfr.fit(X_train_scaled, y_train_scaled)\n",
        "y_pred = Rfr.predict(X_test_scaled)\n",
        "y_pred_rescaled = Target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)\n",
        "score = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "print('R-squared score for the test set: ', round(score,4))\n"
      ],
      "id": "mI4PsdRxG0w_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lPtLE_IG3so"
      },
      "source": [
        "###Polynomial Regression"
      ],
      "id": "2lPtLE_IG3so"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBq3R7I2G3Oy"
      },
      "source": [
        "pol = PolynomialFeatures (degree = 2)\n",
        "x_pol = pol.fit_transform(X_train)\n",
        "Pol_reg = LinearRegression()\n",
        "Pol_reg.fit(X_train_scaled, y_train_scaled)\n",
        "y_pred = Pol_reg.predict(X_test_scaled)\n",
        "y_pred_rescaled = Target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)\n",
        "score = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "print('R-squared score for the test set: ', round(score,4))"
      ],
      "id": "cBq3R7I2G3Oy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HqGpvrbG8lK"
      },
      "source": [
        "### Basic Neural Network"
      ],
      "id": "2HqGpvrbG8lK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlFFvUA5G74r"
      },
      "source": [
        "model_nn = keras.Sequential([\n",
        "  # the hidden layer\n",
        "   layers.Dense(64, activation='sigmoid'),\n",
        "    # the linear output layer \n",
        "    layers.Dense(units=1, input_shape=[X_train_scaled.shape[1]])\n",
        "])\n",
        "model_nn.compile(loss= 'mean_squared_error', optimizer='adam')\n",
        "history_nn = model_nn.fit(X_train_scaled, y_train_scaled, epochs=50)\n",
        "\n",
        "y_pred = model_nn.predict(X_test_scaled)\n",
        "y_pred_rescaled = Target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)\n",
        "score = r2_score(y_test_rescaled, y_pred_rescaled)\n",
        "print('R-squared score for the test set: ', round(score,4))"
      ],
      "id": "NlFFvUA5G74r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9umzCpBHOqa"
      },
      "source": [
        "###LSTM RNN"
      ],
      "id": "Q9umzCpBHOqa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWv6GoMwHTBB"
      },
      "source": [
        "#### Preprocessing for LSTM RNN"
      ],
      "id": "DWv6GoMwHTBB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XiL2FRTHObO"
      },
      "source": [
        "X_train_lstm = np.reshape(X_train_scaled,(X_train_scaled.shape[0], X_train_scaled.shape[1],1) )\n",
        "y_train_lstm = np.reshape(y_train_scaled, (y_train_scaled.shape[0]))\n",
        "\n",
        "X_valid_lstm = np.reshape(X_valid_scaled,(X_valid_scaled.shape[0], X_valid_scaled.shape[1],1) )\n",
        "y_valid_lstm = np.reshape(y_valid_scaled, (y_valid_scaled.shape[0]))\n",
        "\n",
        "X_test_lstm = np.reshape(X_test_scaled,(X_test_scaled.shape[0], X_test_scaled.shape[1],1) )\n",
        "y_test_lstm = np.reshape(y_test_scaled, (y_test_scaled.shape[0]))\n"
      ],
      "id": "9XiL2FRTHObO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q9jrOz6HX5s"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)))\n",
        "\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units = 1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.summary()"
      ],
      "id": "_Q9jrOz6HX5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tFZ46huHcFy"
      },
      "source": [
        "model.fit(x=X_train_lstm, y=y_train_lstm, batch_size=5, epochs=30, verbose=1, validation_data=(X_valid_lstm, y_valid_lstm), shuffle=True)"
      ],
      "id": "4tFZ46huHcFy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw2JpMTwHxAr"
      },
      "source": [
        "loss_per_epoch = model.history.history['loss']\n",
        "val_loss_per_epoch = model.history.history['val_loss']"
      ],
      "id": "Hw2JpMTwHxAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEM3vNsbHzgR"
      },
      "source": [
        "y_pred = model.predict(X_test_lstm)\n",
        "y_pred_rescaled = Target_scaler.inverse_transform(y_pred)\n",
        "y_test_rescaled =  Target_scaler.inverse_transform(y_test_scaled)\n",
        "score = r2_score(y_test_rescaled, y_pred_rescaled)"
      ],
      "id": "wEM3vNsbHzgR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeYwaPHvHfzT"
      },
      "source": [
        "#### Plotting loss values ##can skip this if we are not using LSTM "
      ],
      "id": "OeYwaPHvHfzT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWfbtkLeHej1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(loss_per_epoch);\n",
        "plt.plot(val_loss_per_epoch);\n",
        "plt.title(\"LSTM model loss in MSE\");\n",
        "plt.ylabel(\"loss\");\n",
        "plt.xlabel(\"Epochs\");\n",
        "plt.legend(['train', 'val']);"
      ],
      "id": "CWfbtkLeHej1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3JXrUTH1V2"
      },
      "source": [
        "y_axis = list(df_new_1.index)[-854:]\n",
        "y_actual = pd.DataFrame(y_test_rescaled, columns=['Actual'])\n",
        "y_hat = pd.DataFrame(y_pred_rescaled, columns=['Predicted'])\n",
        "positions = [0,100,200,300,400,500,600,700,800]\n",
        "selected_labels = []\n",
        "for i in positions:\n",
        "  selected_labels.append(y_axis[i])\n",
        "\n",
        "plt.figure(figsize=(18, 10))\n",
        "plt.plot(y_actual, linestyle='solid', color='r')\n",
        "plt.plot(y_hat, linestyle='dashed', color='b')\n",
        "plt.xticks(positions, selected_labels)\n",
        "plt.legend(['Actual','Predicted'], loc='best', prop={'size': 14})\n",
        "plt.title('SSTA in test', weight='bold', fontsize=16)\n",
        "plt.grid(color = 'y', linewidth='0.5')\n",
        "plt.show()"
      ],
      "id": "bQ3JXrUTH1V2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFlyLJqUHyub"
      },
      "source": [
        ""
      ],
      "id": "QFlyLJqUHyub",
      "execution_count": null,
      "outputs": []
    }
  ]
}