{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charged-snowboard",
   "metadata": {},
   "source": [
    "# World Data League 2021\n",
    "## Notebook Template\n",
    "\n",
    "This notebook is one of the mandatory deliverables when you submit your solution (alongside the video pitch). Its structure follows the WDL evaluation criteria and it has dedicated cells where you can add descriptions. Make sure your code is readable as it will be the only technical support the jury will have to evaluate your work.\n",
    "\n",
    "The notebook must:\n",
    "\n",
    "*   üíª have all the code that you want the jury to evaluate\n",
    "*   üß± follow the predefined structure\n",
    "*   üìÑ have markdown descriptions where you find necessary\n",
    "*   üëÄ be saved with all the output that you want the jury to see\n",
    "*   üèÉ‚Äç‚ôÇÔ∏è be runnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-contamination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-03T10:46:52.146959Z",
     "start_time": "2021-07-03T10:46:52.128960Z"
    }
   },
   "source": [
    "## Authors\n",
    "- Nicholas Sistovaris\n",
    "- Moritz Geiger\n",
    "- Pravalika Myneni\n",
    "- Sowmya Madela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-florida",
   "metadata": {},
   "source": [
    "## External links and resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-finland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-03T10:56:16.294657Z",
     "start_time": "2021-07-03T10:56:16.286392Z"
    }
   },
   "source": [
    "All the external data or resources that was not provided by the WDL was acquired through the following links:\n",
    "\n",
    "1. https://noise-planet.org/noisemodelling.html \n",
    "2. https://www.torinocitylab.it/en/asset-to/open-data \n",
    "3. https://www.officeholidays.com/countries/italy/turin/2018 \n",
    "4. https://www.feiertagskalender.ch/index.php?geo=3815&jahr=2018&hl=en\n",
    "5. http://webgis.arpa.piemonte.it/basicviewer_arpa_webapp/index.html?webmap=89aa175451d24ae0a1911e67957d9aec\n",
    "6. http://aperto.comune.torino.it/dataset/zone-statistiche\n",
    "7. https://openweathermap.org/history\n",
    "8. https://developers.google.com/maps/documentation/places/web-service/details "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-ecology",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-hypothetical",
   "metadata": {},
   "source": [
    "**Overview:**\n",
    "\n",
    "\n",
    "_from challenge description_\n",
    "<blockquote>\n",
    "\n",
    "</blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-region",
   "metadata": {},
   "source": [
    "**Research:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-discovery",
   "metadata": {},
   "source": [
    "## Development\n",
    "Start coding here! üë©‚Äçüíª\n",
    "\n",
    "Don't hesitate to create markdown cells to include descriptions of your work where you see fit, as well as commenting your code.\n",
    "\n",
    "We know that you know exactly where to start when it comes to crunching data and building models, but don't forget that WDL is all about social impact...so take that into consideration as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-liability",
   "metadata": {},
   "source": [
    "### Imports (libraries) üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "creative-marketplace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-03T11:17:08.517645Z",
     "start_time": "2021-07-03T11:17:02.250175Z"
    }
   },
   "outputs": [],
   "source": [
    "## TABULAR\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "## GEO\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, HeatMap, BeautifyIcon\n",
    "from folium.map import LayerControl, Layer, FeatureGroup\n",
    "from folium.vector_layers import Circle, CircleMarker\n",
    "from shapely.geometry import LineString, Point\n",
    "from shapely import wkt\n",
    "\n",
    "\n",
    "## DATA\n",
    "import os\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import ast\n",
    "import datetime as dt\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "\n",
    "## VIS\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa\n",
    "import branca\n",
    "import plotly.express as px\n",
    "\n",
    "## TIME SERIES\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima.arima import auto_arima "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-cylinder",
   "metadata": {},
   "source": [
    "### Importing Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-wedding",
   "metadata": {},
   "source": [
    "Following a first glance at the dataframes provided by the WDL, we believed that using data from **2018** was our best bet to construct our model on. \n",
    "\n",
    "- First, we wanted to focus on understanding noise and complaints in the pre-covid context. The years 2020 and 2021 would have been unrepresentative of Turin's nightlife.\n",
    "\n",
    "- Secondly, we wanted a feature that would represent the number of people outsides on an hourly basis. The data on No. of Visitors based on WiFi was most complete and representative of the population outside. However, it only had data for October, November & December 2018. This is why we picked 2018 for the rest of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "valuable-theorem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>address</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>streaming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s_01</td>\n",
       "      <td>Via Saluzzo, 26 Torino</td>\n",
       "      <td>45,059172</td>\n",
       "      <td>7,678986</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s_02</td>\n",
       "      <td>Via Principe Tommaso, 18bis Torino</td>\n",
       "      <td>45,057837</td>\n",
       "      <td>7,681555</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s_03</td>\n",
       "      <td>Largo Saluzzo Torino</td>\n",
       "      <td>45,058518</td>\n",
       "      <td>7,678854</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s_05</td>\n",
       "      <td>Via Principe Tommaso angolo via Baretti Torino</td>\n",
       "      <td>45,057603</td>\n",
       "      <td>7,681348</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s_06</td>\n",
       "      <td>Corso Marconi, 27 Torino</td>\n",
       "      <td>45,055554</td>\n",
       "      <td>7,68259</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                                         address        Lat      Long  \\\n",
       "0  s_01                          Via Saluzzo, 26 Torino  45,059172  7,678986   \n",
       "1  s_02              Via Principe Tommaso, 18bis Torino  45,057837  7,681555   \n",
       "2  s_03                            Largo Saluzzo Torino  45,058518  7,678854   \n",
       "3  s_05  Via Principe Tommaso angolo via Baretti Torino  45,057603  7,681348   \n",
       "4  s_06                        Corso Marconi, 27 Torino  45,055554   7,68259   \n",
       "\n",
       "                                           streaming  \n",
       "0  https://userportal.smartdatanet.it/userportal/...  \n",
       "1  https://userportal.smartdatanet.it/userportal/...  \n",
       "2  https://userportal.smartdatanet.it/userportal/...  \n",
       "3  https://userportal.smartdatanet.it/userportal/...  \n",
       "4  https://userportal.smartdatanet.it/userportal/...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# location of the sensors\n",
    "df_sensors_def = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/noise_sensor_list.csv', sep=';')\n",
    "df_sensors_def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-pavilion",
   "metadata": {},
   "source": [
    "**Note** The location of sensors was optimized to cover all\n",
    "significant feature of ‚ÄúMovida‚Äù area:\n",
    "one in a very crowded square (S_03, not active in\n",
    "daytime), three in narrow streets with pubs and\n",
    "bars (S_01, S_04, S_05), one in a boulevard for\n",
    "traffic noise measurement (S_06) and the last one\n",
    "in a quieter area with no crowd and low traffic\n",
    "(S_02), for global reference. The choice of points\n",
    "of installation was driven also by the power\n",
    "supply, so light poles, public offices and bike\n",
    "sharing station where preferred.\n",
    "\n",
    "Source: https://wdl-data.fra1.digitaloceanspaces.com/torino/120_Euronoise2018.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "optional-caribbean",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>No. of Visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-24 17:00</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-24 18:00</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-24 19:00</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-24 20:00</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-24 21:00</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>2018-12-31 19:00</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>2018-12-31 20:00</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>2018-12-31 21:00</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>2018-12-31 22:00</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>2018-12-31 23:00</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1639 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time  No. of Visitors\n",
       "0     2018-10-24 17:00               47\n",
       "1     2018-10-24 18:00              155\n",
       "2     2018-10-24 19:00              181\n",
       "3     2018-10-24 20:00              211\n",
       "4     2018-10-24 21:00              239\n",
       "...                ...              ...\n",
       "1634  2018-12-31 19:00              158\n",
       "1635  2018-12-31 20:00              171\n",
       "1636  2018-12-31 21:00              151\n",
       "1637  2018-12-31 22:00              125\n",
       "1638  2018-12-31 23:00              168\n",
       "\n",
       "[1639 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wifi = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/WIFI%20Count.csv', sep=',')\n",
    "df_wifi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-entertainment",
   "metadata": {},
   "source": [
    "**Note** As you can see, from the data above, we can get an idea of the number of people outside at different hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beneficial-ground",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WKT</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>OPEN YEAR</th>\n",
       "      <th>OPEN MONTH</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>Description</th>\n",
       "      <th>Merchandise Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (1396322.217 4990301.69)</td>\n",
       "      <td>VIA CLAUDIO LUIGI BERTHOLLET 24</td>\n",
       "      <td>1977</td>\n",
       "      <td>1</td>\n",
       "      <td>EXTRALIMENTARI</td>\n",
       "      <td>PICCOLE STRUTTURE</td>\n",
       "      <td>Extralimentari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (1396322.217 4990301.69)</td>\n",
       "      <td>VIA CLAUDIO LUIGI BERTHOLLET 24</td>\n",
       "      <td>1985</td>\n",
       "      <td>6</td>\n",
       "      <td>ALIMENTARI</td>\n",
       "      <td>PICCOLE STRUTTURE</td>\n",
       "      <td>Panificio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (1396303.762 4990325.001)</td>\n",
       "      <td>VIA CLAUDIO LUIGI BERTHOLLET 25/F</td>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>ALTRO</td>\n",
       "      <td>DIA di somministrazione</td>\n",
       "      <td>Nessuna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (1396434.395 4990540.6)</td>\n",
       "      <td>CORSO VITTORIO EMANUELE II 21/A</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>ALTRO</td>\n",
       "      <td>DIA di somministrazione</td>\n",
       "      <td>Nessuna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (1396434.395 4990540.6)</td>\n",
       "      <td>CORSO VITTORIO EMANUELE II 21/A</td>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>ALTRO</td>\n",
       "      <td>DIA di somministrazione</td>\n",
       "      <td>Nessuna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               WKT                            ADDRESS  \\\n",
       "0   POINT (1396322.217 4990301.69)    VIA CLAUDIO LUIGI BERTHOLLET 24   \n",
       "1   POINT (1396322.217 4990301.69)    VIA CLAUDIO LUIGI BERTHOLLET 24   \n",
       "2  POINT (1396303.762 4990325.001)  VIA CLAUDIO LUIGI BERTHOLLET 25/F   \n",
       "3    POINT (1396434.395 4990540.6)    CORSO VITTORIO EMANUELE II 21/A   \n",
       "4    POINT (1396434.395 4990540.6)    CORSO VITTORIO EMANUELE II 21/A   \n",
       "\n",
       "   OPEN YEAR  OPEN MONTH            TYPE              Description  \\\n",
       "0       1977           1  EXTRALIMENTARI        PICCOLE STRUTTURE   \n",
       "1       1985           6      ALIMENTARI        PICCOLE STRUTTURE   \n",
       "2       2017           9           ALTRO  DIA di somministrazione   \n",
       "3       2013          10           ALTRO  DIA di somministrazione   \n",
       "4       2009           2           ALTRO  DIA di somministrazione   \n",
       "\n",
       "  Merchandise Type  \n",
       "0   Extralimentari  \n",
       "1        Panificio  \n",
       "2          Nessuna  \n",
       "3          Nessuna  \n",
       "4          Nessuna  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_businesses = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/businesses.csv', sep=';')\n",
    "df_businesses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-flashing",
   "metadata": {},
   "source": [
    "**Note** Location & Description of various businesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tamil-interaction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>data_da</th>\n",
       "      <th>data_a</th>\n",
       "      <th>numero_presenze</th>\n",
       "      <th>layer_id</th>\n",
       "      <th>layer_nome</th>\n",
       "      <th>dettaglio(secondi)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-06-10T21:00:00Z</td>\n",
       "      <td>2018-06-10T22:00:00Z</td>\n",
       "      <td>3278</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-06-10T20:00:00Z</td>\n",
       "      <td>2018-06-10T21:00:00Z</td>\n",
       "      <td>3324</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-06-10T19:00:00Z</td>\n",
       "      <td>2018-06-10T20:00:00Z</td>\n",
       "      <td>3318</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-06-10T18:00:00Z</td>\n",
       "      <td>2018-06-10T19:00:00Z</td>\n",
       "      <td>3187</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-06-10T17:00:00Z</td>\n",
       "      <td>2018-06-10T18:00:00Z</td>\n",
       "      <td>2980</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster               data_da                data_a  numero_presenze  \\\n",
       "0  Presenze  2018-06-10T21:00:00Z  2018-06-10T22:00:00Z             3278   \n",
       "1  Presenze  2018-06-10T20:00:00Z  2018-06-10T21:00:00Z             3324   \n",
       "2  Presenze  2018-06-10T19:00:00Z  2018-06-10T20:00:00Z             3318   \n",
       "3  Presenze  2018-06-10T18:00:00Z  2018-06-10T19:00:00Z             3187   \n",
       "4  Presenze  2018-06-10T17:00:00Z  2018-06-10T18:00:00Z             2980   \n",
       "\n",
       "                               layer_id layer_nome  dettaglio(secondi)  \n",
       "0  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "1  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "2  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "3  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "4  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sim_june = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/sim_count/SIM_count_04_100618.csv', sep=';', encoding='latin-1')\n",
    "df_sim_june.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handy-serum",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>data_da</th>\n",
       "      <th>data_a</th>\n",
       "      <th>numero_presenze</th>\n",
       "      <th>layer_id</th>\n",
       "      <th>layer_nome</th>\n",
       "      <th>dettaglio(secondi)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-01-21T22:00:00Z</td>\n",
       "      <td>2018-01-21T23:00:00Z</td>\n",
       "      <td>3026</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-01-21T21:00:00Z</td>\n",
       "      <td>2018-01-21T22:00:00Z</td>\n",
       "      <td>3088</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-01-21T20:00:00Z</td>\n",
       "      <td>2018-01-21T21:00:00Z</td>\n",
       "      <td>3119</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-01-21T19:00:00Z</td>\n",
       "      <td>2018-01-21T20:00:00Z</td>\n",
       "      <td>3114</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-01-21T18:00:00Z</td>\n",
       "      <td>2018-01-21T19:00:00Z</td>\n",
       "      <td>2991</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster               data_da                data_a  numero_presenze  \\\n",
       "0  Presenze  2018-01-21T22:00:00Z  2018-01-21T23:00:00Z             3026   \n",
       "1  Presenze  2018-01-21T21:00:00Z  2018-01-21T22:00:00Z             3088   \n",
       "2  Presenze  2018-01-21T20:00:00Z  2018-01-21T21:00:00Z             3119   \n",
       "3  Presenze  2018-01-21T19:00:00Z  2018-01-21T20:00:00Z             3114   \n",
       "4  Presenze  2018-01-21T18:00:00Z  2018-01-21T19:00:00Z             2991   \n",
       "\n",
       "                               layer_id layer_nome  dettaglio(secondi)  \n",
       "0  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "1  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "2  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "3  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "4  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sim_jan = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/sim_count/SIM_count_15_210118.csv', sep=';', encoding='latin-1')\n",
    "df_sim_jan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "national-dominant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>data_da</th>\n",
       "      <th>data_a</th>\n",
       "      <th>numero_presenze</th>\n",
       "      <th>layer_id</th>\n",
       "      <th>layer_nome</th>\n",
       "      <th>dettaglio(secondi)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-03-25T21:00:00Z</td>\n",
       "      <td>2018-03-25T22:00:00Z</td>\n",
       "      <td>3267</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-03-25T20:00:00Z</td>\n",
       "      <td>2018-03-25T21:00:00Z</td>\n",
       "      <td>3373</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-03-25T19:00:00Z</td>\n",
       "      <td>2018-03-25T20:00:00Z</td>\n",
       "      <td>3410</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-03-25T18:00:00Z</td>\n",
       "      <td>2018-03-25T19:00:00Z</td>\n",
       "      <td>3358</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Presenze</td>\n",
       "      <td>2018-03-25T17:00:00Z</td>\n",
       "      <td>2018-03-25T18:00:00Z</td>\n",
       "      <td>3229</td>\n",
       "      <td>5491d6d2-0c9e-47b7-bfde-c84c632efacc</td>\n",
       "      <td>Area 1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cluster               data_da                data_a  numero_presenze  \\\n",
       "0  Presenze  2018-03-25T21:00:00Z  2018-03-25T22:00:00Z             3267   \n",
       "1  Presenze  2018-03-25T20:00:00Z  2018-03-25T21:00:00Z             3373   \n",
       "2  Presenze  2018-03-25T19:00:00Z  2018-03-25T20:00:00Z             3410   \n",
       "3  Presenze  2018-03-25T18:00:00Z  2018-03-25T19:00:00Z             3358   \n",
       "4  Presenze  2018-03-25T17:00:00Z  2018-03-25T18:00:00Z             3229   \n",
       "\n",
       "                               layer_id layer_nome  dettaglio(secondi)  \n",
       "0  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "1  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "2  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "3  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  \n",
       "4  5491d6d2-0c9e-47b7-bfde-c84c632efacc     Area 1                3600  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sim_march = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/sim_count/SIM_count_19_250318.csv', sep=';', encoding='latin-1')\n",
    "df_sim_march.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unauthorized-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_all = pd.concat([df_sim_jan, df_sim_march, df_sim_june], axis=0)\n",
    "df_sim_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-liabilities",
   "metadata": {},
   "source": [
    "**Note** Another possibility to estimate the number of people outside at certain hours is the SIM card dataframes. What it highlights is the presence of certain SIM card users at different hours of the day. We have access to SIM card data of 2018 for January, March and June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chronic-romance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Ora</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5,,,,,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2018</td>\n",
       "      <td>00:00</td>\n",
       "      <td>68,7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76,0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66,6,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-01-2018</td>\n",
       "      <td>01:00</td>\n",
       "      <td>68,3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68,2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65,4,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-01-2018</td>\n",
       "      <td>02:00</td>\n",
       "      <td>59,8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64,4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64,4,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-01-2018</td>\n",
       "      <td>03:00</td>\n",
       "      <td>67,4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61,8,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-01-2018</td>\n",
       "      <td>04:00</td>\n",
       "      <td>68,0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64,5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60,5,,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data    Ora    C1   C2    C3   C4 C5,,,,,\n",
       "0  01-01-2018  00:00  68,7  NaN  76,0  NaN  66,6,,\n",
       "1  01-01-2018  01:00  68,3  NaN  68,2  NaN  65,4,,\n",
       "2  01-01-2018  02:00  59,8  NaN  64,4  NaN  64,4,,\n",
       "3  01-01-2018  03:00  67,4  NaN  67,5  NaN  61,8,,\n",
       "4  01-01-2018  04:00  68,0  NaN  64,5  NaN  60,5,,"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noise_2018 = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/noise_data/san_salvario_2018.csv', skiprows= [0,1,2,3,4,5,6,7], sep =';')\n",
    "df_noise_2018.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-advocacy",
   "metadata": {},
   "source": [
    "**Note** The noise data is records of noice measurements using 5 different sensors spread in the San Salvario region on an hourly basis. We will use this data as our target in our time series measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southern-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria criminologa</th>\n",
       "      <th>Sottocategoria Criminologica</th>\n",
       "      <th>Circoscrizione</th>\n",
       "      <th>Localita</th>\n",
       "      <th>Area Verde</th>\n",
       "      <th>Data</th>\n",
       "      <th>Ora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>6.0</td>\n",
       "      <td>BELMONTE/(VIA)                                ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/02/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>6.0</td>\n",
       "      <td>DONATORE DI SANGUE/(PIAZZA DEL)               ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/02/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CIBRARIO/LUIGI (VIA)                          ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26/02/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ROMA/(VIA)                                    ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02/03/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ZUMAGLIA/(VIA)                                ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/03/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categoria criminologa Sottocategoria Criminologica  Circoscrizione  \\\n",
       "0       Allarme Sociale                        Altro             6.0   \n",
       "1       Allarme Sociale                        Altro             6.0   \n",
       "2       Allarme Sociale                        Altro             4.0   \n",
       "3       Allarme Sociale                        Altro             1.0   \n",
       "4       Allarme Sociale                        Altro             4.0   \n",
       "\n",
       "                                            Localita Area Verde        Data  \\\n",
       "0  BELMONTE/(VIA)                                ...        NaN  01/02/2018   \n",
       "1  DONATORE DI SANGUE/(PIAZZA DEL)               ...        NaN  12/02/2018   \n",
       "2  CIBRARIO/LUIGI (VIA)                          ...        NaN  26/02/2018   \n",
       "3  ROMA/(VIA)                                    ...        NaN  02/03/2018   \n",
       "4  ZUMAGLIA/(VIA)                                ...        NaN  05/03/2018   \n",
       "\n",
       "   Ora  \n",
       "0  NaN  \n",
       "1  NaN  \n",
       "2  NaN  \n",
       "3  NaN  \n",
       "4  NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_police_1 = pd.read_excel('https://github.com/McNickSisto/world_data_league/blob/main/stage_final/data/police_complaints/OpenDataContact_Gennaio_Giugno_2018.xlsx?raw=true')\n",
    "df_police_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inclusive-renaissance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria criminologa</th>\n",
       "      <th>Sottocategoria Criminologica</th>\n",
       "      <th>Circoscrizione</th>\n",
       "      <th>Localita</th>\n",
       "      <th>Area Verde</th>\n",
       "      <th>Data</th>\n",
       "      <th>Ora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>8.0</td>\n",
       "      <td>D'AZEGLIO/MASSIMO (CORSO)                     ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16/07/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>1.0</td>\n",
       "      <td>REGINA MARGHERITA/(CORSO)                     ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/07/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>10.0</td>\n",
       "      <td>DUINO/(VIA)                                   ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14/09/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02/10/2018</td>\n",
       "      <td>9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>9.0</td>\n",
       "      <td>CARDUCCI/GIOSUE' (PIAZZA)                     ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27/11/2018</td>\n",
       "      <td>11.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categoria criminologa Sottocategoria Criminologica  Circoscrizione  \\\n",
       "0       Allarme Sociale                        Altro             8.0   \n",
       "1       Allarme Sociale                        Altro             1.0   \n",
       "2       Allarme Sociale                        Altro            10.0   \n",
       "3       Allarme Sociale                        Altro             NaN   \n",
       "4       Allarme Sociale                        Altro             9.0   \n",
       "\n",
       "                                            Localita Area Verde        Data  \\\n",
       "0  D'AZEGLIO/MASSIMO (CORSO)                     ...        NaN  16/07/2018   \n",
       "1  REGINA MARGHERITA/(CORSO)                     ...        NaN  17/07/2018   \n",
       "2  DUINO/(VIA)                                   ...        NaN  14/09/2018   \n",
       "3                                                NaN        NaN  02/10/2018   \n",
       "4  CARDUCCI/GIOSUE' (PIAZZA)                     ...        NaN  27/11/2018   \n",
       "\n",
       "     Ora  \n",
       "0    NaN  \n",
       "1    NaN  \n",
       "2    NaN  \n",
       "3   9.40  \n",
       "4  11.53  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_police_2 = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/police_complaints/OpenDataContact_Luglio_Dicembre_2018.csv')\n",
    "df_police_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "frozen-poland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria criminologa</th>\n",
       "      <th>Sottocategoria Criminologica</th>\n",
       "      <th>Circoscrizione</th>\n",
       "      <th>Localita</th>\n",
       "      <th>Area Verde</th>\n",
       "      <th>Data</th>\n",
       "      <th>Ora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>6.0</td>\n",
       "      <td>BELMONTE/(VIA)                                ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/02/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>6.0</td>\n",
       "      <td>DONATORE DI SANGUE/(PIAZZA DEL)               ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/02/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CIBRARIO/LUIGI (VIA)                          ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26/02/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ROMA/(VIA)                                    ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02/03/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allarme Sociale</td>\n",
       "      <td>Altro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ZUMAGLIA/(VIA)                                ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/03/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Qualit√† Urbana</td>\n",
       "      <td>Decoro e degrado urbano</td>\n",
       "      <td>6.0</td>\n",
       "      <td>VERCELLI/(CORSO)                              ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31/12/2018</td>\n",
       "      <td>11.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Qualit√† Urbana</td>\n",
       "      <td>Veicoli abbandonati</td>\n",
       "      <td>4.0</td>\n",
       "      <td>BOSELLI/PAOLO (VIA)                           ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/09/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Qualit√† Urbana</td>\n",
       "      <td>Veicoli abbandonati</td>\n",
       "      <td>4.0</td>\n",
       "      <td>PIFFETTI/PIETRO (VIA)                         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22/09/2018</td>\n",
       "      <td>14.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Qualit√† Urbana</td>\n",
       "      <td>Veicoli abbandonati</td>\n",
       "      <td>6.0</td>\n",
       "      <td>FOSSATA/(VIA)                                 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22/09/2018</td>\n",
       "      <td>9.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Qualit√† Urbana</td>\n",
       "      <td>Veicoli abbandonati</td>\n",
       "      <td>9.0</td>\n",
       "      <td>ROSARIO SANTA FE'/(VIA)                       ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16/10/2018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2168 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Categoria criminologa Sottocategoria Criminologica  Circoscrizione  \\\n",
       "0         Allarme Sociale                        Altro             6.0   \n",
       "1         Allarme Sociale                        Altro             6.0   \n",
       "2         Allarme Sociale                        Altro             4.0   \n",
       "3         Allarme Sociale                        Altro             1.0   \n",
       "4         Allarme Sociale                        Altro             4.0   \n",
       "..                    ...                          ...             ...   \n",
       "990        Qualit√† Urbana      Decoro e degrado urbano             6.0   \n",
       "991        Qualit√† Urbana          Veicoli abbandonati             4.0   \n",
       "992        Qualit√† Urbana          Veicoli abbandonati             4.0   \n",
       "993        Qualit√† Urbana          Veicoli abbandonati             6.0   \n",
       "994        Qualit√† Urbana          Veicoli abbandonati             9.0   \n",
       "\n",
       "                                              Localita Area Verde        Data  \\\n",
       "0    BELMONTE/(VIA)                                ...        NaN  01/02/2018   \n",
       "1    DONATORE DI SANGUE/(PIAZZA DEL)               ...        NaN  12/02/2018   \n",
       "2    CIBRARIO/LUIGI (VIA)                          ...        NaN  26/02/2018   \n",
       "3    ROMA/(VIA)                                    ...        NaN  02/03/2018   \n",
       "4    ZUMAGLIA/(VIA)                                ...        NaN  05/03/2018   \n",
       "..                                                 ...        ...         ...   \n",
       "990  VERCELLI/(CORSO)                              ...        NaN  31/12/2018   \n",
       "991  BOSELLI/PAOLO (VIA)                           ...        NaN  17/09/2018   \n",
       "992  PIFFETTI/PIETRO (VIA)                         ...        NaN  22/09/2018   \n",
       "993  FOSSATA/(VIA)                                 ...        NaN  22/09/2018   \n",
       "994  ROSARIO SANTA FE'/(VIA)                       ...        NaN  16/10/2018   \n",
       "\n",
       "       Ora  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  \n",
       "..     ...  \n",
       "990  11.08  \n",
       "991    NaN  \n",
       "992  14.01  \n",
       "993   9.55  \n",
       "994    NaN  \n",
       "\n",
       "[2168 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_police = pd.concat([df_police_1,df_police_2])\n",
    "df_police"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incoming-dairy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temp</th>\n",
       "      <th>winds</th>\n",
       "      <th>rainfall_mm</th>\n",
       "      <th>snowfall_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>2.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  temp     winds  rainfall_mm  snowfall_mm\n",
       "0  2018-01-01 00:00:00  1.04  0.366667       -0.010     2.600000\n",
       "1  2018-01-01 01:00:00  1.09  0.590000        0.009     2.600000\n",
       "2  2018-01-01 02:00:00  1.05  0.450000        0.008     2.266667\n",
       "3  2018-01-01 03:00:00  0.89  0.400000        0.006     2.266667\n",
       "4  2018-01-01 04:00:00  0.73  0.780000       -0.011     2.300000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather = pd.read_csv(\"https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/all_weather.csv\")\n",
    "df_weather = df_weather.drop(columns = ['Unnamed: 0'])\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-consortium",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "See details in [Appendix](#Weather Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "central-velvet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2018</td>\n",
       "      <td>monday</td>\n",
       "      <td>New year's Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06-01-2018</td>\n",
       "      <td>saturday</td>\n",
       "      <td>La Befana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19-03-2018</td>\n",
       "      <td>monday</td>\n",
       "      <td>Father's day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25-03-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>Palm Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-04-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>Easter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>02-04-2018</td>\n",
       "      <td>monday</td>\n",
       "      <td>Easter Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25-04-2018</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>liberation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>01-05-2018</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>Labour day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09-05-2018</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>Europe day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13-05-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>mother's day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20-05-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>Whitsun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>02-06-2018</td>\n",
       "      <td>saturday</td>\n",
       "      <td>republic day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24-06-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>St. John's Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15-07-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>Bernhard II of Baden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15-08-2018</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>Ferragosto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21-09-2018</td>\n",
       "      <td>friday</td>\n",
       "      <td>Matthew the Apostle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28-10-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>DST end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>01-11-2018</td>\n",
       "      <td>thursday</td>\n",
       "      <td>All Saint's Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>02-12-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>first advent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>08-12-2018</td>\n",
       "      <td>saturday</td>\n",
       "      <td>Immaculate Conception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>09-12-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>second advent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16-12-2018</td>\n",
       "      <td>sunday</td>\n",
       "      <td>third advent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25-12-2018</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>christmas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26-12-2018</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>St. Stephen's Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31-12-2018</td>\n",
       "      <td>monday</td>\n",
       "      <td>New year's Eve</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date        Day                Holiday\n",
       "0   01-01-2018     monday         New year's Day\n",
       "1   06-01-2018   saturday              La Befana\n",
       "2   19-03-2018     monday           Father's day\n",
       "3   25-03-2018     sunday            Palm Sunday\n",
       "4   01-04-2018     sunday                 Easter\n",
       "5   02-04-2018     monday          Easter Monday\n",
       "6   25-04-2018  wednesday             liberation\n",
       "7   01-05-2018    tuesday             Labour day\n",
       "8   09-05-2018  wednesday             Europe day\n",
       "9   13-05-2018     sunday           mother's day\n",
       "10  20-05-2018     sunday                Whitsun\n",
       "11  02-06-2018   saturday           republic day\n",
       "12  24-06-2018     sunday         St. John's Day\n",
       "13  15-07-2018     sunday   Bernhard II of Baden\n",
       "14  15-08-2018  wednesday             Ferragosto\n",
       "15  21-09-2018     friday    Matthew the Apostle\n",
       "16  28-10-2018     sunday                DST end\n",
       "17  01-11-2018   thursday        All Saint's Day\n",
       "18  02-12-2018     sunday           first advent\n",
       "19  08-12-2018   saturday  Immaculate Conception\n",
       "20  09-12-2018     sunday          second advent\n",
       "21  16-12-2018     sunday           third advent\n",
       "22  25-12-2018    tuesday              christmas\n",
       "23  26-12-2018  wednesday      St. Stephen's Day\n",
       "24  31-12-2018     monday         New year's Eve"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_holidays = pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/holidays.csv')\n",
    "df_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "advance-paint",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GET MATCHES'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3a23e45d2102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET MATCHES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[0;32m-> 1357\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    640\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GET MATCHES'"
     ]
    }
   ],
   "source": [
    "df_matches = pd.read_csv('GET MATCHES')\n",
    "df_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opening_hours = pd.read_csv('GET OPENING HOURS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-insulin",
   "metadata": {},
   "source": [
    "### Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise1 = df_noise_2018.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noise_2018['date_hour'] = pd.to_datetime(df_noise_2018['date_hour'])\n",
    "df_noise_2018['date_hour'] = df_noise_2018['date_hour'].dt.strftime(\"%d-%m-%y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noise_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wifi.rename(columns = {'Time': 'date_time'}, inplace=True)\n",
    "df_wifi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wifi['date_time'] = pd.to_datetime(df_wifi['date_time'])\n",
    "df_wifi['date_time'] = df_wifi['date_time'].dt.strftime(\"%d-%m-%y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['time'] = pd.to_datetime(df_weather['time'])\n",
    "df_weather['time'] = df_weather['time'].dt.strftime(\"%d-%m-%y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-cradle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for x, line in enumerate(df_sim_all['data_da']):\n",
    "    df_sim_all['data_da'][x] = line[8:10] + line[4:7] + '-' + line[0:4] +' ' + line[11:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_all.rename(columns= {'data_da' : 'date_time'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-connection",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sim_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_all['date_time'] = pd.to_datetime(df_sim_all['date_time'])\n",
    "df_sim_all['date_time'] = df_sim_all['date_time'].dt.strftime(\"%d-%m-%y %H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-grenada",
   "metadata": {},
   "source": [
    "Merging noise, wifi, sim,weather,... police"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_noise_2018.merge(df_wifi, left_on= 'date_hour', right_on= 'date_time', how='left')\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-means",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_1 = df_final.merge(df_sim_all, left_on= 'date_hour', right_on= 'date_time', how='left')\n",
    "df_final_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_2 = df_final_1.merge(df_weather, left_on= 'date_hour', right_on= 'time', how='left')\n",
    "df_final_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_3 = df_final_2.drop(columns = ['date_time_x','date_time_y', 'time'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_3['date_hour'] = pd.to_datetime(df_final_3['date_hour'])\n",
    "df_final_3['date'] = df_final_3['date_hour'].dt.strftime(\"%d-%m-%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finalized = df_final_3.merge(df_holidays, left_on='date', right_on = 'Date', how =\"left\")\n",
    "df_finalized['isHoliday'] = df_finalized['Holiday'].apply(lambda x: 0 if pd.isnull(x)==True else 1)\n",
    "df_finalized.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finalized = df_finalized.drop(columns= ['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-cancer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_finalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finalized.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finalized.to_csv('Noise_weather_wifi_sim_holidays.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-pickup",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise_2018=pd.read_csv('/content/drive/MyDrive/finals/noise_data/san_salvario_2018.csv',skiprows=[0,1,2,3,4,5,6,7],delimiter=';')\n",
    "df=pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/Noise_weather_wifi_sim_holidays.csv')\n",
    "#Converting to date time\n",
    "df['date_hour']=pd.to_datetime(df['date_hour'])\n",
    "df=df.drop(columns=['Unnamed: 0','C1','C2','C3','C4','C5,,,,,','date','Day'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date']=df['date_hour'].dt.date\n",
    "df['hour']=df['date_hour'].dt.hour\n",
    "df['day']=df['date_hour'].dt.dayofweek\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-sheriff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_noise_2018.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise1['Ora']=pd.to_datetime(noise1['Ora']).dt.hour\n",
    "noise1['Data']=pd.to_datetime(noise1['Data']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the noise readings into decimal format\n",
    "noise1['C1']=noise1['C1'].apply(lambda x: str(x).replace(',','.'))\n",
    "noise1['C2']=noise1['C2'].apply(lambda x: str(x).replace(',','.'))\n",
    "noise1['C3']=noise1['C3'].apply(lambda x: str(x).replace(',','.'))\n",
    "noise1['C4']=noise1['C4'].apply(lambda x: str(x).replace(',','.'))\n",
    "noise1['C5']=noise1['C5'].apply(lambda x: str(x).replace(',','.'))\n",
    "#Conerting the noise reading to float values\n",
    "noise1['C1']=noise1['C1'].apply(lambda x: float(x))\n",
    "noise1['C2']=noise1['C2'].apply(lambda x: float(x))\n",
    "noise1['C3']=noise1['C3'].apply(lambda x: float(x))\n",
    "noise1['C4']=noise1['C4'].apply(lambda x: float(x))\n",
    "noise1['C5']=noise1['C5'].apply(lambda x: float(x))\n",
    "noise1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(noise1, df,  how='inner', left_on=['Data','Ora'], right_on = ['date','hour'])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=new_df.drop(columns=['Data','Ora'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fillig the null values considering means on hourly basis\n",
    "new_df[\"C1\"] = new_df.groupby([\"hour\",'day'])['C1'].transform(lambda x: x.fillna(round(x.mean(),1)))\n",
    "new_df[\"C2\"] = new_df.groupby([\"hour\",'day'])['C2'].transform(lambda x: x.fillna(round(x.mean(),1)))\n",
    "new_df[\"C3\"] = new_df.groupby([\"hour\",'day'])['C3'].transform(lambda x: x.fillna(round(x.mean(),1)))\n",
    "new_df[\"C4\"] = new_df.groupby([\"hour\",'day'])['C4'].transform(lambda x: x.fillna(round(x.mean(),1)))\n",
    "new_df[\"C5\"] = new_df.groupby([\"hour\",'day'])['C5'].transform(lambda x: x.fillna(round(x.mean(),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Log_Avg']=np.log10(((10**(new_df['C1']/10))+(10**(new_df['C2']/10))+(10**(new_df['C3']/10))+(10**(new_df['C4']/10))+(10**(new_df['C5']/10)))/5)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_mat = new_df.corr()\n",
    "sns.heatmap(correlation_mat, annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pairs = correlation_mat.unstack()\n",
    "print(corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pairs = corr_pairs.sort_values(kind=\"quicksort\")\n",
    "strong_pairs = sorted_pairs[abs(sorted_pairs) > 0.5]\n",
    "strong_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['data_a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-guest",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_police[df_police['Ora'].isna()] #many complaints do not have hours associated with them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_2018=pd.read_csv('raw_data/noise_data/san_salvario_2018.csv',\n",
    "                       skiprows=8,\n",
    "                       delimiter=';',\n",
    "                      decimal=',',\n",
    "#                       parse_dates=[['Data', 'Ora']],\n",
    "                      )\n",
    "\n",
    "# # workaround for hour concat issue\n",
    "noise_2018['Data'] = pd.to_datetime(noise_2018['Data'], format='%d-%m-%Y', errors='coerce')\n",
    "noise_2018['date_hour'] = noise_2018.apply(lambda x: pd.to_datetime(str(x.Data) + ' ' + str(x.Ora), errors='coerce'), axis=1)\n",
    "noise_2018 = noise_2018.drop(columns=['Data', 'Ora'])\n",
    "\n",
    "\n",
    "noise_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matches with sensor data\n",
    "match = pd.read_csv('raw_data/football/matches_2018.csv', index_col=0).set_index('Date')\n",
    "match['is_match'] = match.is_match.apply(lambda x: x+80 if x == 1 else x)\n",
    "fig = px.line(noise_2018.set_index('date_hour'))\n",
    "fig.add_scatter(x=match.index, \n",
    "                y=match['is_match'], \n",
    "                mode='markers',\n",
    "                name='football match'\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-webster",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-constant",
   "metadata": {},
   "source": [
    "#### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('https://raw.githubusercontent.com/McNickSisto/world_data_league/main/stage_final/data/Imputed_Data_Final.csv')\n",
    "data=data.drop(columns='Unnamed: 0')\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i = data.set_index('date_hour')\n",
    "data_i.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data_i['Log_Avg']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "additive = seasonal_decompose(df,freq=52, model='additive',extrapolate_trend='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "additive_df = pd.concat([additive.seasonal, additive.trend, additive.resid, additive.observed], axis=1)\n",
    "additive_df.columns = ['seasonal', 'trend', 'resid', 'actual_values']\n",
    "additive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize': (20,10)})\n",
    "additive.plot().suptitle('Additive Decompose')\n",
    "#The Trend,residuals are interesting, showing periods of high variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = additive.trend\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(trend.values)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "# Original Series\n",
    "fig, axes = plt.subplots(3, 2, sharex=True)\n",
    "axes[0, 0].plot(trend.values); axes[0, 0].set_title('Original Series')\n",
    "plot_acf(trend.values, ax=axes[0, 1]).suptitle('Original Series', fontsize=0)\n",
    "# 1st Differencing\n",
    "diff1 = trend.diff().dropna()\n",
    "axes[1, 0].plot(diff1.values)\n",
    "axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(diff1.values, ax=axes[1, 1]).suptitle('1st Order Differencing', fontsize=0)\n",
    "# 2nd Differencing\n",
    "diff2 = trend.diff().diff().dropna()\n",
    "axes[2, 0].plot(diff2.values)\n",
    "axes[2, 0].set_title('2nd Order Differencing')\n",
    "plot_acf(diff2.values, ax=axes[2, 1]).suptitle('2nd Order Differencing', fontsize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n",
    "size = 100\n",
    "fig, axes = plt.subplots(1, 2, sharex=True)\n",
    "axes[0].plot(diff1.values[:size])\n",
    "axes[0].set_title('1st Order Differencing')\n",
    "axes[1].set(ylim=(0,5))\n",
    "plot_pacf(diff1.values[:size], lags=50, ax=axes[1]).suptitle('1st Order Differencing', fontsize=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "train = trend[:3000]\n",
    "test  = trend[3000:]\n",
    "# order = (p=1, d=1, q=1)\n",
    "model = ARIMA(train, order=(1, 1, 1))  \n",
    "model = model.fit(disp=0)  \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual errors\n",
    "residuals = pd.DataFrame(model.resid)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "residuals.plot(title=\"Residuals\", ax=ax[0])\n",
    "residuals.plot(kind='kde', title='Density', ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc, se, conf = model.forecast(14311, alpha=0.05)\n",
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc, index=test.index)\n",
    "lower_series = pd.Series(conf[:, 0], index=test.index)\n",
    "upper_series = pd.Series(conf[:, 1], index=test.index)\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(train, label='training')\n",
    "plt.plot(test, label='actual')\n",
    "plt.plot(fc_series, label='forecast')\n",
    "plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                 color='k', alpha=.15)\n",
    "plt.title('Forecast vs Actuals')\n",
    "plt.legend(loc='upper left', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-jason",
   "metadata": {},
   "source": [
    "#### Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-fitting",
   "metadata": {},
   "source": [
    "Moving Average Smoothing is a technique applied to time series to remove the fine-grained variation between time steps. The hope of smoothing is to remove noise and better expose the signal of the underlying causal processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,6)\n",
    "df.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail-rolling average transform\n",
    "rolling = df.rolling(window=3)\n",
    "rolling_mean = rolling.mean()\n",
    "rolling_mean.dropna(inplace= True)\n",
    "print(rolling_mean.head())\n",
    "# plot original and transformed dataset\n",
    "df.plot()\n",
    "rolling_mean.plot(color='lightgreen')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "# prepare situation\n",
    "X = df.values\n",
    "window = 3\n",
    "history = [X[i] for i in range(window)]\n",
    "test = [X[i] for i in range(window, len(X))]\n",
    "predictions = list()\n",
    "# walk forward over time steps in test\n",
    "for t in range(len(test)):\n",
    "\tlength = len(history)\n",
    "\tyhat = mean([history[i] for i in range(length-window,length)])\n",
    "\tobs = test[t]\n",
    "\tpredictions.append(yhat)\n",
    "\thistory.append(obs)\n",
    "\t#print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "# plot\n",
    "pyplot.plot(test)\n",
    "pyplot.plot(predictions, color='lightcoral')\n",
    "pyplot.show()\n",
    "# zoom plot\n",
    "pyplot.plot(test[0:100])\n",
    "pyplot.plot(predictions[0:100], color='lightcoral')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-gateway",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Scalability and Impact\n",
    "Tell us how applicable and scalable your solution is if you were to implement it in a city. Identify possible limitations and measure the potential social impact of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-thread",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "Now picture the following scenario: imagine you could have access to any type of data that could help you solve this challenge even better. What would that data be and how would it improve your solution? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-contamination",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Appendix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-transformation",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-affiliation",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-width",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('raw_data/weather/weather_1.csv',\n",
    "#                       nrows=1000, #rm later\n",
    "                      sep=';',\n",
    "#                       decimal=',',\n",
    "                      skiprows=4,\n",
    "#                       parse_dates=[[0, 1]],\n",
    "#                       dayfirst=True,\n",
    "                      header=0,\n",
    "                      names=['date', 'hour', 'rainfall_mm', 'snowfall_mm'],\n",
    "                     )\n",
    "\n",
    "# workaround for hour concat issue\n",
    "weather['date'] = pd.to_datetime(weather['date'], format='%d-%m-%Y', errors='coerce')\n",
    "weather['date_hour'] = weather.apply(lambda x: pd.to_datetime(str(x.date) + ' ' + str(x.hour), errors='coerce'), axis=1)\n",
    "\n",
    "# workaround for decimal issue\n",
    "weather['rainfall_mm'] = weather.rainfall_mm.apply(lambda x: str(x).replace(',','.'))\n",
    "weather['snowfall_mm'] = weather.snowfall_mm.apply(lambda x: str(x).replace(',','.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-darkness",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather2 = pd.read_csv('raw_data/weather/weather_2.csv', \n",
    "                 sep=';', \n",
    "                 skiprows=4, \n",
    "                 header=0, \n",
    "#                  decimal=',',\n",
    "#                 converters={2:lambda x: x.replace(',', '.')},\n",
    "#                 parse_dates=[[0, 1]],\n",
    "                names=['date', 'hour', 'winds'],\n",
    "                na_values={2:'',\n",
    "                            3:''},\n",
    "                dayfirst=True,\n",
    "                )\n",
    "# workaround for hour concat issue\n",
    "weather2['date'] = pd.to_datetime(weather2['date'], format='%d-%m-%Y', errors='coerce')\n",
    "weather2['date_hour'] = weather2.apply(lambda x: pd.to_datetime(str(x.date) + ' ' + str(x.hour), errors='coerce'), axis=1)\n",
    "\n",
    "weather2['winds'] = weather2.winds.apply(lambda x: str(x).replace(',','.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-meaning",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# weather['date_hour'] = pd.to_datetime(weather['date_hour'], errors='coerce')\n",
    "weather_1 = weather.dropna(subset=['date_hour'])\n",
    "\n",
    "# weather2['date_hour'] = pd.to_datetime(weather2['date_hour'], errors='coerce')\n",
    "weather_2 = weather2.dropna(subset=['date_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-maldives",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_weather = weather_2.merge(weather_1,\n",
    "                                right_on='date_hour',\n",
    "                                left_on='date_hour',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-morrison",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_weather.sort_values(by='date_hour').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-broadway",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_weather['hourly_date'] = merged_weather.date_hour.apply(lambda x: x.floor('h'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-behavior",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_weather = merged_weather.astype({'winds': float,\n",
    "                      'rainfall_mm':float,\n",
    "                      'snowfall_mm':float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-sphere",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hourly_weather = merged_weather.groupby('hourly_date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-nerve",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hourly_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-doctor",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hourly_weather.to_csv('hourly_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-luxury",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hourly_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-effects",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Open Weather Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-repository",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# API KEY\n",
    "load_dotenv(find_dotenv())\n",
    "OWM_API = os.environ.get(\"OWM_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-height",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init time range\n",
    "range_2019 = pd.DataFrame(pd.date_range('2016-06-01', '2021-06-12', freq='h'), columns=['hour'])\n",
    "range_2019.tail().hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-soldier",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "req = 'http://history.openweathermap.org/data/2.5/history/wdl'\n",
    "start = range_2019.hour.min().value\n",
    "inter = range_2019.hour.max().value\n",
    "end = range_2019.hour.max().value\n",
    "# tail1 = tail.min().value\n",
    "# tail2 = tail.max().value\n",
    "params = {\n",
    "    'id':'3165524', # ID of Turin\n",
    "    'type':'hour',\n",
    "    'start':str(start)[:10], # unix time\n",
    "    'end':str(end)[:10],\n",
    "    'appid': OWM_API\n",
    "}\n",
    "\n",
    "r = requests.get(req, params=params)\n",
    "\n",
    "\n",
    "# with open('data/weather.txt', 'w') as outfile:\n",
    "#     json.dump(r.json(), outfile)\n",
    "    \n",
    "weather = r.json()\n",
    "lst = weather.get('list')\n",
    "dct = {x.get('dt'):x.get('weather')[0].get('main') for x in lst}\n",
    "weather_df = pd.DataFrame.from_dict(dct, \n",
    "                                    orient='index', \n",
    "                                    columns=['weather']).reset_index().rename(columns={'index':'time'})\n",
    "weather_df['rain'] = weather_df.weather == 'Rain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-tractor",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lst = weather.get('list')\n",
    "dct = {x.get('dt'):x.get('main').get('temp') for x in lst}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-kenya",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather_df = pd.DataFrame.from_dict(dct, \n",
    "                                    orient='index', \n",
    "                                    columns=['temp']).reset_index().rename(columns={'index':'time'})\n",
    "weather_df['temp'] = weather_df.temp-273.15\n",
    "weather_df['time'] = pd.to_datetime(weather_df.time, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-quebec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-candle",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merge_all = weather_df.merge(hourly_weather, left_on='time', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-biology",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_all.to_csv('all_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-future",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merge_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-antenna",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Matches Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-europe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# API KEY\n",
    "load_dotenv(find_dotenv())\n",
    "FOOTBALL = os.environ.get(\"FOOTBALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-routine",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# headers = {'X-Auth-Token': FOOTBALL}\n",
    "# url = 'https://api.football-data.org/v2/matches'\n",
    "# params = {'dateFrom': '2018-04-14',\n",
    "#          'dateTo': '2018-04-16'}\n",
    "# r = requests.get(url, headers=headers, params=params)\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-charles",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root = 'raw_data/football/'\n",
    "dfs = []\n",
    "for i in os.listdir(root):\n",
    "    if '.csv' in i:\n",
    "        df = pd.read_csv(root+i)\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-airport",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# filter all by juve\n",
    "juve1 = dfs[0][(dfs[0]['HomeTeam'] == 'Juventus') \\\n",
    "              | (dfs[0]['AwayTeam'] == 'Juventus')]['Date']\n",
    "juve1 = pd.to_datetime(juve1, format='%d/%m/%Y')\n",
    "\n",
    "juve2 = dfs[1][(dfs[1]['Home Team'] == 'Juventus') \\\n",
    "               | (dfs[1]['Away Team'] == 'Juventus')]['Date']\n",
    "juve2 = pd.to_datetime(juve2.apply(lambda x: x[:10]), format=\"%d/%m/%Y\")\n",
    "\n",
    "juve3 = dfs[2][(dfs[2]['HomeTeam'] == 'Juventus') \\\n",
    "              | (dfs[2]['AwayTeam'] == 'Juventus')]['Date']\n",
    "juve3 = pd.to_datetime(juve3, format='%d/%m/%y')\n",
    "\n",
    "juve4 = dfs[3][(dfs[3]['Home Team'] == 'Juventus') \\\n",
    "               | (dfs[3]['Away Team'] == 'Juventus')]['Date']\n",
    "juve4 = pd.to_datetime(juve4.apply(lambda x: x[:10]), format=\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-legislature",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# concat all dates\n",
    "all_concat = pd.DataFrame(pd.concat([juve1, juve2, juve3, juve4]))\n",
    "# all_concat['Date'] = pd.to_datetime(all_concat.Date)\n",
    "all_concat['is_match'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-booking",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_concat.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-wyoming",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get all 2018 matches\n",
    "all_concat_2018 = all_concat[(all_concat.Date > '01-01-2018') \\\n",
    "                            & (all_concat.Date < '2018-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-cartoon",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# put in 2018 time series\n",
    "r = pd.date_range('2018-01-01', '2018-12-31', freq='h')\n",
    "matches = all_concat_2018.set_index('Date').reindex(r).rename_axis('Date').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-programming",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-london",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matches.to_csv('raw_data/football/matches_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-uzbekistan",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Opening Hours Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-glenn",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This part is a bit messy, so we will explain: \n",
    "We used the ```nearbysearch``` [Link](https://developers.google.com/maps/documentation/places/web-service/search#PlaceSearchRequests) to get all the ```bars``` and ```restaurants``` business hours. \n",
    "\n",
    "Then we fetch the unique id ```reference``` from the list of businesses and run it through the ```place_details``` API [Link](https://developers.google.com/maps/documentation/places/web-service/details)\n",
    "\n",
    "From there we extract all the ```open``` (time-)elements and ```close``` (time-)elements and stack them in a dataframe divided by days of the week (0-6). \n",
    "\n",
    "In the end we merge the findings with an empty time series of 2018 with an 'hourly' sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-opera",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# API KEY\n",
    "load_dotenv(find_dotenv())\n",
    "GOOGLE = os.environ.get(\"GOOGLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-engineer",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first find all bars\n",
    "\n",
    "url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json'\n",
    "params = {            \n",
    "            'location':'45.05917,7.67899', #sensor\n",
    "            'radius':'200',\n",
    "            'type':'restaurant',\n",
    "            'key':GOOGLE,\n",
    "            'next_page_token':'Aap_uED24ODLIlOhPdAHG7xFrCg_OrsQ_jAruvTm3QSG4Qbnp5Q85Aa4K7ar-QgnGI7Xnl1epc9YIEj17piMfVpFUxQysBwi8XTzdWbtl6IBGKTKQwV_kxhaAUWr8JG6XVo-BVKHd8NJUwiTP-_uQvkKxc5vLZ4-v6T8ZBuS42zw5DE1L2KgNPCbm86EsPhPYOj8L1MXTRdEm_GhmQSdOt8nDxG4gKkbxiXvmHNTmuBLavqN-VrbpkRBBoVZz_t2P53_ShPgndMEwlt55EYlZHCYK2gHymy9WJjMjKn3VzS6CfcTQJ-TjgsxsrRjSqNXV4T5i2qusSJ__gsam11RBY8XRADB31i-ec_wYCh1529gNKKy9tdQbidVaQjAI72wQ-7yzTZXGzxpz8ob_DHkdVdyJLxijWoHqsXY7oQM-W3Db0u08SHwaooMyb3Da9Ij'\n",
    "         } \n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-coaching",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = r.json().get('results')\n",
    "results2 = r.json().get('results')\n",
    "results3 = r.json().get('results')\n",
    "results4 = r.json().get('results')\n",
    "results5 = r.json().get('results')\n",
    "results6 = r.json().get('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-decrease",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bars = results + results2 + results3 + results4 + results5 + results6\n",
    "restaurants = results + results2 + results3 + results4 + results5 + results6\n",
    "len(restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-consciousness",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get specific opening hrs from fetched bars/restaurants\n",
    "url = 'https://maps.googleapis.com/maps/api/place/details/json'\n",
    "params = {\n",
    "    'key':GOOGLE,\n",
    "    'fields':'opening_hours'\n",
    "         }\n",
    "opening_hrs = []\n",
    "for bar in restaurants:\n",
    "    reference = bar.get('reference')\n",
    "    params['place_id'] = reference\n",
    "    r = requests.get(url, params=params)\n",
    "    opening_hrs.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-people",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "contents_hrs = [r.json() for r in opening_hrs]\n",
    "periods = []\n",
    "for x in contents_hrs:\n",
    "    try:\n",
    "        hr = x.get('result').get('opening_hours').get('periods')\n",
    "        periods.append(hr)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-scroll",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove 24h open bars\n",
    "new = [x for x in periods if len(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-agent",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "closing = []\n",
    "for x in new:\n",
    "    for i in x:\n",
    "        _close = i.get('close')\n",
    "        closing.append(_close)\n",
    "opening = []\n",
    "for x in new:\n",
    "    for i in x:\n",
    "        _open = i.get('open')\n",
    "        opening.append(_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-bandwidth",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opening_times_rest = pd.DataFrame(opening)\n",
    "closing_times_rest = pd.DataFrame(closing)\n",
    "closing_times_rest['time'] = pd.to_datetime(closing_times_rest['time'], format='%H%M')\n",
    "opening_times_rest['time'] = pd.to_datetime(opening_times_rest['time'], format='%H%M')\n",
    "closing_times_rest['day'] = closing_times_rest.day.apply(lambda x: x-1 if x != 0 else 6)\n",
    "opening_times_rest['day'] = opening_times_rest.day.apply(lambda x: x-1 if x != 0 else 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-outreach",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create unique day_hr identifier\n",
    "closing_times_rest['day_time'] = closing_times_rest.apply(lambda x: str(x.day) + '_' + str(x.time.hour), axis=1)\n",
    "opening_times_rest['day_time'] = opening_times_rest.apply(lambda x: str(x.day) + '_' + str(x.time.hour), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-practitioner",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# put results in dataframe\n",
    "opening_times = pd.DataFrame(opening)\n",
    "closing_times = pd.DataFrame(closing)\n",
    "closing_times['time'] = pd.to_datetime(closing_times['time'], format='%H%M')\n",
    "opening_times['time'] = pd.to_datetime(opening_times['time'], format='%H%M')\n",
    "closing_times['day'] = closing_times.day.apply(lambda x: x-1 if x != 0 else 6)\n",
    "opening_times['day'] = opening_times.day.apply(lambda x: x-1 if x != 0 else 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-silly",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create unique day_hr identifier\n",
    "closing_times['day_time'] = closing_times.apply(lambda x: str(x.day) + '_' + str(x.time.hour), axis=1)\n",
    "opening_times['day_time'] = opening_times.apply(lambda x: str(x.day) + '_' + str(x.time.hour), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-sarah",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "closing_all = pd.concat([closing_times_rest, closing_times])\n",
    "opening_all = pd.concat([opening_times_rest, opening_times])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-arrest",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# count all apperances of openings and closings per weekday\n",
    "agg_close = closing_all.groupby('day_time').agg({'day':'count'}).rename(columns={'day':'count_close'})\n",
    "agg_open = opening_all.groupby('day_time').agg({'day':'count'}).rename(columns={'day':'count_open'})\n",
    "agg_joint = agg_close.join(agg_open, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-wrist",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# init range 2018\n",
    "range_2018 = pd.DataFrame(pd.date_range('2018-01-01', '2018-12-31', freq='h'), columns=['hour'])\n",
    "range_2018['day_time'] =  range_2018.apply(lambda x: str(x.hour.weekday()) + '_' + str(x.hour.hour), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-techno",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# join both\n",
    "\n",
    "opening_count_2018 = range_2018.merge(agg_joint, \n",
    "                                    on='day_time',\n",
    "                                    how='left').drop(columns='day_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-knife",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opening_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-henry",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opening_count_2018.sort_values(by='count_open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-drawing",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opening_count_2018.to_csv('raw_data/opening_count_2018.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
