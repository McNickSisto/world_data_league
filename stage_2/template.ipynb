{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeP6xkRIBJco"
   },
   "source": [
    "# World Data League 2021\n",
    "## Notebook Template\n",
    "\n",
    "This notebook is one of the mandatory deliverables when you submit your solution (alongside the video pitch). Its structure follows the WDL evaluation criteria and it has dedicated cells where you can add descriptions. Make sure your code is readable as it will be the only technical support the jury will have to evaluate your work.\n",
    "\n",
    "The notebook must:\n",
    "\n",
    "*   üíª have all the code that you want the jury to evaluate\n",
    "*   üß± follow the predefined structure\n",
    "*   üìÑ have markdown descriptions where you find necessary\n",
    "*   üëÄ be saved with all the output that you want the jury to see\n",
    "*   üèÉ‚Äç‚ôÇÔ∏è be runnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QNcZrkVu9xf"
   },
   "source": [
    "## External links and resources\n",
    "Paste here all the links to external resources that are necessary to understand and run your code. Add descriptions to make it clear how to use them during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJzSXXIYvxf9"
   },
   "source": [
    "1. Risk profile of streets = https://wdl-data.fra1.digitaloceanspaces.com/pse/m_risk_prfile.zip\n",
    "2. Excel explaining the categorical features : https://wdl-data.fra1.digitaloceanspaces.com/pse/Dictionary_Risk_Profiles.xlsx\n",
    "2. OSM map = https://download.bbbike.org/osm/extract/planet_-9.89,38.265_-8.309,39.136.osm.pbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63ltgxp_rOpI"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp34gOznrwrq"
   },
   "source": [
    "\n",
    "When it comes to road safety, Portugal has one of the less impressive records in Europe, however, authorities have been taking steps in an attempt to improve the statistics; with fatalities dropping by 40% since 2010. Despite this, more than 400 people lost their lives in 2017 in road accidents and more than 40,000 injured.\n",
    "\n",
    "An **European report** underlined these characteristics: \n",
    "- In Portugal, relatively many moped riders, lorry and truck occupants died in road accidents compared to the EU average.\n",
    "- Portugal has a somewhat higher share of male road fatalities than the EU average.\n",
    "- Fatalities in built-up areas, during daylight and while raining are overrepresented in Portugal.\n",
    "- The number of speed tickets per population in Portugal is much lower than the EU average\n",
    "\n",
    "Furthermore from our analysis we could see that there are three environments that where the pavement properties significantly, yet distinctly, influence the occurrence of accidents:\n",
    "\n",
    "1. Rural environment with a heavy presence of urban characteristics\n",
    "2. Environment characterized by a considerable predominance of intersections in a rural environment\n",
    "3. Environment with curved segments, high longitudinal gradients and **average speed higher than the tolerable speed**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYPHOTHESIS** :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8rCpNajszur"
   },
   "source": [
    "## Development\n",
    "Start coding here! üë©‚Äçüíª\n",
    "\n",
    "Don't hesitate to create markdown cells to include descriptions of your work where you see fit, as well as commenting your code.\n",
    "\n",
    "We know that you know exactly where to start when it comes to crunching data and building models, but don't forget that WDL is all about social impact...so take that into consideration as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afB4W0KnutpV"
   },
   "source": [
    "### IMPORTING PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J4jPpcKOutZ5"
   },
   "outputs": [],
   "source": [
    "# GENERAL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# LOADING DATA\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "import osmium\n",
    "import fiona\n",
    "import json\n",
    "\n",
    "\n",
    "# GEOSPATIAL DATA\n",
    "from shapely.geometry import Point, LineString, MultiPoint\n",
    "from shapely.geometry import shape \n",
    "\n",
    "\n",
    "# PLOTTING DATA\n",
    "from folium import Map, CircleMarker, Vega, Popup, Marker, PolyLine, Icon, Choropleth, LayerControl\n",
    "from folium.plugins import MarkerCluster, HeatMap, BeautifyIcon\n",
    "from folium.features import ColorLine, GeoJsonPopup, GeoJsonTooltip\n",
    "from folium.map import FeatureGroup\n",
    "import shapely\n",
    "import matplotlib\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "\n",
    "# STATS\n",
    "import math\n",
    "import stats\n",
    "\n",
    "# ML\n",
    "import scipy\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQ90XveiBN6A"
   },
   "source": [
    "### Extracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to start clean and analyses the dataset given by the ***WDL*** team: a shape file containing 34678 different road segments. Each of these road segments is characterized by information on traffic intensity, velocity and environment in which this is inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34678, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testing remote opening of files\n",
    "# constructor of google download links\n",
    "dl_construct = 'https://drive.google.com/uc?export=download&id='\n",
    "# id from share link google drive\n",
    "file_id = '1m2BpnJ-NXqlqFW8gYnC_fEI2PLTrXz1r'\n",
    "geo_df = gpd.read_file(f'{dl_construct}{file_id}')\n",
    "geo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_df(geo):\n",
    "    df = pd.DataFrame(geo).drop(columns='Link_ID')\n",
    "    df_ren = df.rename(columns={\n",
    "                        'Daily_Aver':'Daily_Average_Traffic_Intensity',\n",
    "                        'Average_Ve':'Average_Velocity_of_Vehicle_Traffic',\n",
    "                        'Median_of_':'Median_of_velocity_of_Vehicle_Traffic',\n",
    "                        'First_Quar': 'FirstQuartil_of_velocity_of_Vehicle_Traffic',\n",
    "                        'Third_Quar': 'ThirdQuartil_of_velocity_of_Vehicle_Traffic'\n",
    "                    })\n",
    "    return df_ren\n",
    "    \n",
    "df = first_df(geo_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better understanding on our data and to avoid errors during our analysis we need to investigate it with general statistics.\n",
    "To be able to trust our analysis we have to clean the dataset before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linkid</th>\n",
       "      <th>Daily_Average_Traffic_Intensity</th>\n",
       "      <th>Average_Velocity_of_Vehicle_Traffic</th>\n",
       "      <th>Median_of_velocity_of_Vehicle_Traffic</th>\n",
       "      <th>FirstQuartil_of_velocity_of_Vehicle_Traffic</th>\n",
       "      <th>ThirdQuartil_of_velocity_of_Vehicle_Traffic</th>\n",
       "      <th>Func_Class</th>\n",
       "      <th>Speed_Cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.467800e+04</td>\n",
       "      <td>34678.000000</td>\n",
       "      <td>34678.000000</td>\n",
       "      <td>34678.000000</td>\n",
       "      <td>34678.000000</td>\n",
       "      <td>34678.000000</td>\n",
       "      <td>34678.000000</td>\n",
       "      <td>34678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.958206e+08</td>\n",
       "      <td>3340.417942</td>\n",
       "      <td>56.816834</td>\n",
       "      <td>56.463409</td>\n",
       "      <td>43.822041</td>\n",
       "      <td>68.091844</td>\n",
       "      <td>2.684613</td>\n",
       "      <td>4.904781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.355910e+08</td>\n",
       "      <td>2725.873982</td>\n",
       "      <td>51.983670</td>\n",
       "      <td>26.240876</td>\n",
       "      <td>24.442204</td>\n",
       "      <td>30.985191</td>\n",
       "      <td>0.538658</td>\n",
       "      <td>1.520568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.021682e+07</td>\n",
       "      <td>14.435864</td>\n",
       "      <td>-401.703724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-392.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.364832e+08</td>\n",
       "      <td>1903.398108</td>\n",
       "      <td>38.315321</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.067377e+08</td>\n",
       "      <td>2644.529317</td>\n",
       "      <td>49.966126</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>38.875000</td>\n",
       "      <td>60.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.154997e+09</td>\n",
       "      <td>3897.886608</td>\n",
       "      <td>69.511585</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.223731e+09</td>\n",
       "      <td>49309.806935</td>\n",
       "      <td>6357.022296</td>\n",
       "      <td>1326.250000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>2605.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             linkid  Daily_Average_Traffic_Intensity  \\\n",
       "count  3.467800e+04                     34678.000000   \n",
       "mean   8.958206e+08                      3340.417942   \n",
       "std    2.355910e+08                      2725.873982   \n",
       "min    8.021682e+07                        14.435864   \n",
       "25%    7.364832e+08                      1903.398108   \n",
       "50%    9.067377e+08                      2644.529317   \n",
       "75%    1.154997e+09                      3897.886608   \n",
       "max    1.223731e+09                     49309.806935   \n",
       "\n",
       "       Average_Velocity_of_Vehicle_Traffic  \\\n",
       "count                         34678.000000   \n",
       "mean                             56.816834   \n",
       "std                              51.983670   \n",
       "min                            -401.703724   \n",
       "25%                              38.315321   \n",
       "50%                              49.966126   \n",
       "75%                              69.511585   \n",
       "max                            6357.022296   \n",
       "\n",
       "       Median_of_velocity_of_Vehicle_Traffic  \\\n",
       "count                           34678.000000   \n",
       "mean                               56.463409   \n",
       "std                                26.240876   \n",
       "min                                 1.000000   \n",
       "25%                                38.250000   \n",
       "50%                                50.000000   \n",
       "75%                                71.000000   \n",
       "max                              1326.250000   \n",
       "\n",
       "       FirstQuartil_of_velocity_of_Vehicle_Traffic  \\\n",
       "count                                 34678.000000   \n",
       "mean                                     43.822041   \n",
       "std                                      24.442204   \n",
       "min                                    -392.500000   \n",
       "25%                                      26.000000   \n",
       "50%                                      38.875000   \n",
       "75%                                      56.000000   \n",
       "max                                     143.000000   \n",
       "\n",
       "       ThirdQuartil_of_velocity_of_Vehicle_Traffic    Func_Class     Speed_Cat  \n",
       "count                                 34678.000000  34678.000000  34678.000000  \n",
       "mean                                     68.091844      2.684613      4.904781  \n",
       "std                                      30.985191      0.538658      1.520568  \n",
       "min                                       1.000000      1.000000      2.000000  \n",
       "25%                                      48.000000      2.000000      4.000000  \n",
       "50%                                      60.333333      3.000000      6.000000  \n",
       "75%                                      85.000000      3.000000      6.000000  \n",
       "max                                    2605.000000      3.000000      7.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regarding the columns we know that they report values in km/h: many of the min and max we can observe thank to describe function don't make sense. \n",
    "- We need to operate on them as they are **outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers:\n",
    "def rm_out(df):\n",
    "    for i in df.columns.drop(['linkid', 'Daily_Average_Traffic_Intensity','geometry']):\n",
    "        lb = 0\n",
    "        ub = 180\n",
    "#         print(lb, ub)\n",
    "        df[i] = df[i].mask(df[i] < lb) \n",
    "        df[i] = df[i].mask(df[i] > ub) \n",
    "    return df\n",
    "\n",
    "data = rm_out(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we run describe again we will see that the data regarding velocity has just feasible values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linkid</th>\n",
       "      <th>Daily_Average_Traffic_Intensity</th>\n",
       "      <th>Average_Velocity_of_Vehicle_Traffic</th>\n",
       "      <th>Median_of_velocity_of_Vehicle_Traffic</th>\n",
       "      <th>FirstQuartil_of_velocity_of_Vehicle_Traffic</th>\n",
       "      <th>ThirdQuartil_of_velocity_of_Vehicle_Traffic</th>\n",
       "      <th>Func_Class</th>\n",
       "      <th>Speed_Cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.467800e+04</td>\n",
       "      <td>34678.000000</td>\n",
       "      <td>34633.000000</td>\n",
       "      <td>34675.000000</td>\n",
       "      <td>34677.000000</td>\n",
       "      <td>34674.000000</td>\n",
       "      <td>34678.000000</td>\n",
       "      <td>34678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.958206e+08</td>\n",
       "      <td>3340.417942</td>\n",
       "      <td>56.112805</td>\n",
       "      <td>56.402803</td>\n",
       "      <td>43.834624</td>\n",
       "      <td>67.959767</td>\n",
       "      <td>2.684613</td>\n",
       "      <td>4.904781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.355910e+08</td>\n",
       "      <td>2725.873982</td>\n",
       "      <td>24.346245</td>\n",
       "      <td>25.054113</td>\n",
       "      <td>24.329987</td>\n",
       "      <td>26.706718</td>\n",
       "      <td>0.538658</td>\n",
       "      <td>1.520568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.021682e+07</td>\n",
       "      <td>14.435864</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.364832e+08</td>\n",
       "      <td>1903.398108</td>\n",
       "      <td>38.317003</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.067377e+08</td>\n",
       "      <td>2644.529317</td>\n",
       "      <td>49.961538</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>38.875000</td>\n",
       "      <td>60.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.154997e+09</td>\n",
       "      <td>3897.886608</td>\n",
       "      <td>69.447459</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.223731e+09</td>\n",
       "      <td>49309.806935</td>\n",
       "      <td>179.691892</td>\n",
       "      <td>143.250000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             linkid  Daily_Average_Traffic_Intensity  \\\n",
       "count  3.467800e+04                     34678.000000   \n",
       "mean   8.958206e+08                      3340.417942   \n",
       "std    2.355910e+08                      2725.873982   \n",
       "min    8.021682e+07                        14.435864   \n",
       "25%    7.364832e+08                      1903.398108   \n",
       "50%    9.067377e+08                      2644.529317   \n",
       "75%    1.154997e+09                      3897.886608   \n",
       "max    1.223731e+09                     49309.806935   \n",
       "\n",
       "       Average_Velocity_of_Vehicle_Traffic  \\\n",
       "count                         34633.000000   \n",
       "mean                             56.112805   \n",
       "std                              24.346245   \n",
       "min                               1.000000   \n",
       "25%                              38.317003   \n",
       "50%                              49.961538   \n",
       "75%                              69.447459   \n",
       "max                             179.691892   \n",
       "\n",
       "       Median_of_velocity_of_Vehicle_Traffic  \\\n",
       "count                           34675.000000   \n",
       "mean                               56.402803   \n",
       "std                                25.054113   \n",
       "min                                 1.000000   \n",
       "25%                                38.250000   \n",
       "50%                                50.000000   \n",
       "75%                                71.000000   \n",
       "max                               143.250000   \n",
       "\n",
       "       FirstQuartil_of_velocity_of_Vehicle_Traffic  \\\n",
       "count                                 34677.000000   \n",
       "mean                                     43.834624   \n",
       "std                                      24.329987   \n",
       "min                                       0.000000   \n",
       "25%                                      26.000000   \n",
       "50%                                      38.875000   \n",
       "75%                                      56.000000   \n",
       "max                                     143.000000   \n",
       "\n",
       "       ThirdQuartil_of_velocity_of_Vehicle_Traffic    Func_Class     Speed_Cat  \n",
       "count                                 34674.000000  34678.000000  34678.000000  \n",
       "mean                                     67.959767      2.684613      4.904781  \n",
       "std                                      26.706718      0.538658      1.520568  \n",
       "min                                       1.000000      1.000000      2.000000  \n",
       "25%                                      48.000000      2.000000      4.000000  \n",
       "50%                                      60.333333      3.000000      6.000000  \n",
       "75%                                      85.000000      3.000000      6.000000  \n",
       "max                                     164.000000      3.000000      7.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) == len(data.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ***no duplicates*** in our dataset: NO ACTION NEEDED\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Average_Velocity_of_Vehicle_Traffic            45\n",
       " ThirdQuartil_of_velocity_of_Vehicle_Traffic     4\n",
       " Median_of_velocity_of_Vehicle_Traffic           3\n",
       " FirstQuartil_of_velocity_of_Vehicle_Traffic     1\n",
       " linkid                                          0\n",
       " Daily_Average_Traffic_Intensity                 0\n",
       " Func_Class                                      0\n",
       " Speed_Cat                                       0\n",
       " geometry                                        0\n",
       " dtype: int64,\n",
       " 'Total of data points : 34678')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sort_values(ascending=False) , f'Total of data points : {data.shape[0]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the total highest number of missing values detected for column is 45 on a total number of rows of 34'678.\n",
    "- The missing value for Average Velocity e the ones in Speed Difference Mean are the same (one column is created from the other one)\n",
    "- The missing values of other column can be easily deleted\n",
    "\n",
    "**As we are handling data regarding AVERAGE velocity we can easily substitute the missing values with the mean of the corresponding column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_missing(data):\n",
    "    imputer = SimpleImputer()\n",
    "    data['Average_Velocity_of_Vehicle_Traffic']=imputer.fit_transform(data[['Average_Velocity_of_Vehicle_Traffic']])\n",
    "    return data.dropna()\n",
    "data = handling_missing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linkid                                         0\n",
       "Daily_Average_Traffic_Intensity                0\n",
       "Average_Velocity_of_Vehicle_Traffic            0\n",
       "Median_of_velocity_of_Vehicle_Traffic          0\n",
       "FirstQuartil_of_velocity_of_Vehicle_Traffic    0\n",
       "ThirdQuartil_of_velocity_of_Vehicle_Traffic    0\n",
       "Func_Class                                     0\n",
       "Speed_Cat                                      0\n",
       "geometry                                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature creation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at our data we need to search for a target that in the next step we will use in our model.\n",
    "Most common causes of Accidents:\n",
    "- Over Speeding.\n",
    "- Drunken Driving.\n",
    "- Distractions to Driver.\n",
    "- Red Light Jumping.\n",
    "- Avoiding Safety Gears like Seat belts and Helmets.\n",
    "- Non-adherence to lane driving and overtaking in a wrong manner.\n",
    "\n",
    "The first cause is always the **over-speed** that can be connected with one of the above causes. \n",
    "For this reason we decide to investigate and use as target information regarding the velocity.\n",
    "\n",
    "- Speed_Cat (described in the excel below)\n",
    "- Average Velocity of Vehicle Traffic \n",
    "- Median of velocity of Vehicle Traffic\n",
    "\n",
    "We will create a dictionary that, from the information contained in the excel can describe the type of street regarding the max velocity allowed in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_construct = 'https://drive.google.com/uc?export=download&id='\n",
    "# id from share link google drive\n",
    "file_id = '1m2BpnJ-NXqlqFW8gYnC_fEI2PLTrXz1r'\n",
    "request = requests.get(f'{dl_construct}{file_id}').content\n",
    "memory = BytesIO(request)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File is not a recognized excel file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3446fe2dec73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspeed_explanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SpeedCat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspeed_explanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1072\u001b[0m                     \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(path, content, storage_options)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZIP_SIGNATURE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a recognized excel file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;31m# ZipFile typing is overly-strict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File is not a recognized excel file"
     ]
    }
   ],
   "source": [
    "speed_explanation = pd.read_excel(memory, sheet_name='SpeedCat')\n",
    "speed_explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the table above we can create a dictionary.\n",
    "1. count values for category\n",
    "2. translate the speed range in actual number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Speed_Cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NO need of mapping for label 1 and 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_speed_dict = {2:130,3:100,4:90,5:70,6:50,7:30}\n",
    "def target_creation(data):\n",
    "    data['Max_speed'] = data['Speed_Cat'].map(max_speed_dict)\n",
    "    data['Speed_Diff_Mean'] = data['Max_speed'] - data['Average_Velocity_of_Vehicle_Traffic']\n",
    "    data['Speed_Diff_Median'] = data['Max_speed'] - data['Median_of_velocity_of_Vehicle_Traffic']\n",
    "    return data\n",
    "data = target_creation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = {x:data[data['Max_speed']==x] for x in max_speed_dict.values()}\n",
    "def speed_dist(cat_list):\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "    fig.suptitle('Categorical Distributions', size=20)\n",
    "#     fig.tight_layout()\n",
    "    for c, i in enumerate(cat_list.items()):\n",
    "        speedlim = i[0]\n",
    "        plt.subplot(3, 2, c+1)\n",
    "#         plt.set_xlabel('Average Speed')\n",
    "        ax = i[1].Average_Velocity_of_Vehicle_Traffic.hist(bins=30)\n",
    "        ylim = i[1].Average_Velocity_of_Vehicle_Traffic.value_counts(bins=30).max()\n",
    "        ax.set_title(f'Speed Category {i[0]} kph', size=13)\n",
    "        ax.set_xlabel('Average Speed')\n",
    "        ax.axvline(speedlim, color='r', linestyle='--')\n",
    "#         print(f'done {speedlim}, {ylim/2}, {speedlim}')\n",
    "        ax.text(x=speedlim+7, y=float(ylim-ylim/4), s=f'Speed Limit: {speedlim}')\n",
    "        \n",
    "speed_dist(cat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*<br>\n",
    "We can see that most over speeding is taking place at roads with lower speed limits such as *50 kph and 30 kph.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Over speeding behavior can be extracted by the deltas between the road's speed category and its actual average speed observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Speed_Cat','Max_speed', 'Speed_Diff_Mean','Speed_Diff_Median']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that the main reason that is able to decrease the safety of a street is the speed rate.\n",
    "**Our first target will be the difference between the mean of velocity and the max speed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to scale our dataframe to have a distribution *around* the mean.\n",
    "\n",
    "1. We need to separate numerical and categorical column\n",
    "2. We are going to use the Min-Max Scaling method for the numerical ones: is the one that is commonly used distance based algorithms, as k-means that is one of the possible analysis we are taking in consideration.  \n",
    "3. For the categorical ones we'll use the OneHotEncoding method (for each label in each category creates a different column)\n",
    "\n",
    "We could also operate this step all together but is important for us to know which column belong to each of the different classes inside the categorical feature. \n",
    "**To do so we need to operate for each categorical separately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_numerical(data):\n",
    "    numerical = data.columns.drop(['geometry','linkid','Speed_Cat', 'Func_Class'])\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = data.copy()\n",
    "    for column in numerical:\n",
    "        scaler.fit(data_scaled[[column]])\n",
    "        data_scaled[column]=scaler.transform(data_scaled[[column]]) \n",
    "    return data_scaled\n",
    "data_scaled = scaling_numerical(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **BEFORE SCALING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['geometry','linkid','Speed_Cat', 'Func_Class']).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **AFTER SCALING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.drop(columns=['geometry','linkid','Speed_Cat', 'Func_Class']).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with the categorical features the first thing we need to do is to understand the distribution within the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Func_Class.value_counts() , data.Speed_Cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functional Class has just 3 possible label for the street that we can understand better looking at the excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_explanation = pd.read_excel('wdl_dict/Dictionary_Risk_Profiles.xlsx', sheet_name='Func_Class')\n",
    "for i,el in enumerate(func_explanation['Description']):\n",
    "    print(f'Class n.{i+1} : {el} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new and deeper understanding of the distribution and the meaning of the category (*NB: regarding speed_cat we can look back at the point **1.1.5 \"Feature creation\"** to get these informations)* we can now progress with our transformations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_categorical(data):\n",
    "    ohe = OneHotEncoder(sparse = False)\n",
    "    ohe.fit(data[['Func_Class']])\n",
    "    func_encoded = ohe.transform(data[['Func_Class']])\n",
    "    data[\"func_1\"],data[\"func_2\"],data['func_3'] = func_encoded.T\n",
    "    ohe = OneHotEncoder(sparse = False)\n",
    "    ohe.fit(data[['Speed_Cat']])\n",
    "    speed_encoded = ohe.transform(data[['Speed_Cat']])\n",
    "    data[\"speed_2\"],data[\"speed_3\"],data[\"speed_4\"],\\\n",
    "    data[\"speed_5\"], data[\"speed_6\"], data[\"speed_7\"]= speed_encoded.T\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = scaling_categorical(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed Dataframe: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the first dataframe, the one given by the challenge, completely ready for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that ***over speeding*** is the main reason for road hazards. Over speeding behavior can be extracted by the deltas between the road's speed category and its actual average speed observations as processed in column ```Speed_Diff_Mean```. <br>\n",
    "\n",
    "Over speeding can be harnessed among others by the roads environment [Source](https://www.tandfonline.com/doi/abs/10.1080/014416499295420). People chose their speeding behavior not only by speed limits but also by their assessment of the road's quality and the surrounding environment.<br>\n",
    "\n",
    "Therefore we chose to gather more information about POIs, amenities and public buildings in the surrounding of the provided road segments. Those can be acquired through OSM sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data set to Lisbon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "waiting expl sisto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming pandas df to geopandas df\n",
    "geo_df = gpd.GeoDataFrame(data_scaled)\n",
    "geo_df.geometry[0].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering only lisbon data inside the circle of 38.72526068747401, -9.142352617846093 with buffer '1'\n",
    "circle_lisbon = Point(-9.142352617846093, 38.72526068747401).buffer(1)\n",
    "geo_lis = geo_df[geo_df.geometry.within(circle_lisbon)]\n",
    "# no immediate usage of this pd.DataFrame\n",
    "df_lis = pd.DataFrame(geo_lis).drop(columns=['geometry', 'linkid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The new data set has {df_lis.shape[0]} rows as opposed to the original set with {geo_df.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading OSM Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://download.bbbike.org/osm/extract/planet_-9.89,38.265_-8.309,39.136.osm.pbf \\\n",
    "    --quiet -O map_data/Lisbon.osm.pbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ogrinfo map_data/Lisbon.osm.pbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ogr2ogr -f \"GPKG\" \\\n",
    "     map_data/lisbon_polygons.gpkg \\\n",
    "     map_data/Lisbon.osm.pbf \\\n",
    "    -nlt POLYGONS \\\n",
    "    -nln polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "# about 3 mins\n",
    "layer_file = \"map_data/lisbon_polygons.gpkg\"\n",
    "collection = list(fiona.open(layer_file,'r'))\n",
    "df1 = pd.DataFrame(collection)\n",
    "\n",
    "#Check Geometry\n",
    "def isvalid(geom):\n",
    "    try:\n",
    "        shape(geom)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df1['isvalid'] = df1['geometry'].apply(lambda x: isvalid(x))\n",
    "df1 = df1[df1['isvalid'] == 1]\n",
    "collection = json.loads(df1.to_json(orient='records'))\n",
    "\n",
    "#Convert to geodataframe\n",
    "gdf_lis_poly = gpd.GeoDataFrame.from_features(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lis_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_gdf = gdf_lis_poly.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading POIs from pre-processed OSM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The data set of POIs in the Lisbon region has {poi_gdf.shape[0]} individual points which can be merged with our data set.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_gdf.geometry.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***<br>\n",
    "For now we will only be focussing on the geometrical points in the OSM data, not on polygons or line strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering down to shapely.geometry.Points\n",
    "gdf_points = poi_gdf[poi_gdf['geometry'].type == 'Point'].reset_index()\n",
    "gdf_points.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**<br>\n",
    "The points provided are categorized and stored in many columns. We will shrink this information to one column and fill it with all the important information about the point. <br>\n",
    "Some points do not provide any information. Those ones will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing geo_df columns, only leaving one valid column\n",
    "def new_desc(geo):\n",
    "    geo['desc_points'] = None\n",
    "    # columns to be taken into consideration\n",
    "    lst_cols = [  'amenity', \n",
    "                  'barrier', \n",
    "                  'building', \n",
    "                  'highway', \n",
    "                  'landuse', \n",
    "                  'man_made', \n",
    "                  'natural', \n",
    "                  'office']\n",
    "    for c, row in geo.iterrows():\n",
    "#         concat_name = [f'feat_{i}_{row[i]}' for i in lst_cols if row[i] == row[i]]\n",
    "        concat_name = [f'feat_{i}_{row[i]}' for i in lst_cols if row[i] != None]\n",
    "        if len(concat_name) > 0:\n",
    "            geo.at[c, 'desc_points'] = concat_name[0]\n",
    "        else: \n",
    "            geo.at[c, 'desc_points'] = None\n",
    "        print(f'done: {c}')\n",
    "        \n",
    "    \n",
    "    geo = geo[['geometry', 'desc_points']]\n",
    "    # drop empty descriptions\n",
    "    geo = geo.dropna(subset=['desc_points'])\n",
    "    geo= geo.reset_index(drop=True)\n",
    "    \n",
    "    return geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying cleaning function to geo df\n",
    "gdf_points_clean = new_desc(gdf_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_points_clean.head(5)\n",
    "# only two columns are left => geometry and name of point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-transforming point's names into columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the dataset of points for the merger with the general data set we need to re-transfer the unique feature names into columns. In total we have **96** feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all unique values\n",
    "encoder = OneHotEncoder()\n",
    "enc_df = encoder.fit_transform(gdf_points_clean[['desc_points']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reapplying column names\n",
    "enc_gdf_points = gpd.GeoDataFrame(enc_df.toarray(), columns=encoder.categories_[0])\n",
    "enc_gdf_points = enc_gdf_points.join(gdf_points_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**<br>\n",
    "We need the ```desc_points``` column for later plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_gdf_points.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Points with Road segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to merge the points with the provided road segments we need to buffer the LineStrings of the roads and turn them into little Polygons in order to overlap with the POIs around the road. Later we will use the ```.intersect``` method for spatial joins to keep only the points which are in the vicinity of the road segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gdf with buffered road segments\n",
    "geo_lis_buf = geo_lis.copy()\n",
    "# allowing certain buffer to road segments to \"catch\" the points. buffer=.0005 seems to be visually adequate.\n",
    "geo_lis_buf['geometry'] = geo_lis_buf.geometry.buffer(.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining both geo dfs\n",
    "joint_gpd = gpd.sjoin(enc_gdf_points, geo_lis_buf, how=\"inner\", op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We have {joint_gpd.shape[0]} intersecting points with our road segments.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**<br>\n",
    "Now, we want to regroup the GDF back to our initial granularity, the road segments with unique link_IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the aggregation dictionary for the .groupby method\n",
    "columns = joint_gpd.columns\n",
    "agg = {i:'max' for i in columns if 'feat' in i}\n",
    "agg['geometry'] = lambda x: list(x)\n",
    "agg['desc_points'] = lambda x: list(x)\n",
    "# all added features should not be summed per road segment, they will be scaled down to 1 item max.\n",
    "# all geometries and point descriptions should be listed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**<br>\n",
    "We **do not want** to aggregate the POIs as summed values. This would add a high bias to the model. E.g. traffic lights or toll booths appear more than once on one road segment. The model would under interpret their function and meaning if they weren't scaled down **to one unit** per segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regrouping by linkid\n",
    "grouped_gpd = joint_gpd.groupby('linkid').agg(agg)\n",
    "# renaming the 'geometry' column so that the gdf won't be confused later\n",
    "grouped_gpd = grouped_gpd.rename(columns={'geometry':'points'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**<br>\n",
    "Only road segments which contained one or more points will be left in the gdf. That's the nature of the inner ```sjoin```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_lis = geo_lis.merge(grouped_gpd, left_on='linkid', right_index=True)\n",
    "geo_df_lis['point_count'] = geo_df_lis['points'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a glance at the merged df\n",
    "pd.set_option('display.max_columns', None)\n",
    "geo_df_lis.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_lis = geo_df_lis.sort_values(by='linkid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_lis[geo_df_lis['linkid']==80264692]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testplots for the merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_amount = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set of points to make them readable for the Marker (folium) (```point.coords.xy[1][0]```, ```point.coords.xy[0][0]```)\n",
    "- only display ONE POI per section. all name duplicates will be overwritten. Example 'toll booths'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_points = []\n",
    "for c, row in geo_df_lis.iterrows():\n",
    "    name_ix = {name:c for c, name in enumerate(row.desc_points)}\n",
    "    name_point = {name:(row.points[c].coords.xy[1][0], row.points[c].coords.xy[0][0]) for name, c in name_ix.items()}\n",
    "    lst_points.append(name_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lis_buf = geo_df_lis.copy()\n",
    "map_lis_buf['geometry'] = map_lis_buf.geometry.buffer(.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init map\n",
    "m = Map([38.74288, -9.16624], zoom_start=13)\n",
    "\n",
    "# unpacking list of lists containing points, mapping them to their names\n",
    "## POINTS\n",
    "marker_cluster = MarkerCluster(name='Points')\n",
    "# unpacking single points out of list of dictionaries and assigning to marker_cluster\n",
    "for pairs in lst_points[:plot_amount]:\n",
    "    popups = [Popup(f'<p><b>Name:</b></p> <p>{a}</p>', max_width=200) for a in pairs.keys()]\n",
    "    markers = [Marker(coord, popup=popups[c]).add_to(marker_cluster) for c, coord in enumerate(pairs.values())]\n",
    "\n",
    "print('built points')\n",
    "\n",
    "## BUFFERED ROADS (enable via layer control)\n",
    "buf_layer = FeatureGroup(name=\"roads_buf\", show=False)\n",
    "roads_buf = Choropleth(geo_data=map_lis_buf.head(plot_amount)[['linkid', 'Speed_Diff_Mean', 'geometry']],\n",
    "                       data=None,\n",
    "                       highlight=True,\n",
    "                      ).geojson.add_child(GeoJsonTooltip(['linkid']))\n",
    "\n",
    "\n",
    "## ROADS\n",
    "roads = Choropleth(geo_data=geo_df_lis.head(plot_amount).geometry,\n",
    "                          data=None, \n",
    "                          name=\"roads\", \n",
    "                          show=True)\n",
    "\n",
    "print('built roads')\n",
    "\n",
    "marker_cluster.add_to(m)\n",
    "roads_buf.add_to(buf_layer)\n",
    "buf_layer.add_to(m)\n",
    "roads.add_to(m)\n",
    "\n",
    "LayerControl().add_to(m)\n",
    "print('activate buffered roads via layer control on upper right corner')\n",
    "print(f'this is only a subset of the whole df with {plot_amount} rows.')\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**<br>\n",
    "The displayed points all seem to be within the boundaries of the buffered road segments üõ£. The ```sjoin``` with ```intersect``` is working.<br>\n",
    "NOT ALL points are displayed. That would take up too much memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**<br>\n",
    "Stripping the df to only relevant features. Geometries only serve the plotting on a map.<br>\n",
    "Avg. velocity and road cat can be removed due to feature creation in 1.4.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(geo_df_lis.copy().drop(columns=['Average_Velocity_of_Vehicle_Traffic', \n",
    "                                                          'Median_of_velocity_of_Vehicle_Traffic',\n",
    "                                                          'FirstQuartil_of_velocity_of_Vehicle_Traffic',\n",
    "                                                          'ThirdQuartil_of_velocity_of_Vehicle_Traffic',\n",
    "                                                          'Speed_Diff_Median', ##### CHECK IT OUT. ONE HAS TO GO\n",
    "                                                          'Func_Class',\n",
    "                                                          'Speed_Cat',\n",
    "                                                          'Max_speed',\n",
    "                                                          'geometry',\n",
    "                                                          'points', \n",
    "                                                          'desc_points', \n",
    "                                                          'point_count']))\n",
    "print(final_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSDath2nr1fq"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "### Scalability and Impact\n",
    "Tell us how applicable and scalable your solution is if you were to implement it in a city. Identify possible limitations and measure the potential social impact of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGmbES9GszEv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XBiBOyAl2Sv"
   },
   "source": [
    "### Future Work\n",
    "Now picture the following scenario: imagine you could have access to any type of data that could help you solve this challenge even better. What would that data be and how would it improve your solution? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gK3heTKl7qz"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Notebook Submission Template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
